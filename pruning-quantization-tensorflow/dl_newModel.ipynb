{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.1\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "\n",
    "print(tf.__version__)\n",
    "print(len(tf.config.experimental.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#IMPORTAÇÃO E NORRMALIZAÇÃO\n",
    "(x_train, y_train), (x_test,y_test) = keras.datasets.cifar10.load_data()\n",
    "#x_train = x_train.reshape(-1, 28, 28, 1)\n",
    "#x_test = x_test.reshape(-1, 28, 28, 1)\n",
    "\n",
    "x_train = x_train.astype(float)/255\n",
    "x_test = x_test.astype(float)/255\n",
    "\n",
    "#CRIAR DATASET\n",
    "train_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(50000).batch(64)\n",
    "test_ds = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False)\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.1)\n",
    "acc_metric = keras.metrics.SparseCategoricalAccuracy()\n",
    "test_accuracy = tf.keras.metrics.Accuracy()\n",
    "test_loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "train_accuracy = tf.keras.metrics.Accuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"modelo-0.0\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1 (Conv2D)               (64, 32, 32, 32)          896       \n",
      "_________________________________________________________________\n",
      "bn1 (BatchNormalization)     (64, 32, 32, 32)          128       \n",
      "_________________________________________________________________\n",
      "conv2 (Conv2D)               (64, 32, 32, 32)          9248      \n",
      "_________________________________________________________________\n",
      "bn2 (BatchNormalization)     (64, 32, 32, 32)          128       \n",
      "_________________________________________________________________\n",
      "mp1 (MaxPooling2D)           (64, 16, 16, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv3 (Conv2D)               (64, 16, 16, 64)          18496     \n",
      "_________________________________________________________________\n",
      "bn3 (BatchNormalization)     (64, 16, 16, 64)          256       \n",
      "_________________________________________________________________\n",
      "conv4 (Conv2D)               (64, 16, 16, 64)          36928     \n",
      "_________________________________________________________________\n",
      "bn4 (BatchNormalization)     (64, 16, 16, 64)          256       \n",
      "_________________________________________________________________\n",
      "mp2 (MaxPooling2D)           (64, 8, 8, 64)            0         \n",
      "_________________________________________________________________\n",
      "conv5 (Conv2D)               (64, 8, 8, 128)           73856     \n",
      "_________________________________________________________________\n",
      "bn5 (BatchNormalization)     (64, 8, 8, 128)           512       \n",
      "_________________________________________________________________\n",
      "conv6 (Conv2D)               (64, 8, 8, 128)           147584    \n",
      "_________________________________________________________________\n",
      "bn6 (BatchNormalization)     (64, 8, 8, 128)           512       \n",
      "_________________________________________________________________\n",
      "mp3 (MaxPooling2D)           (64, 4, 4, 128)           0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (64, 2048)                0         \n",
      "_________________________________________________________________\n",
      "dp1 (Dropout)                (64, 2048)                0         \n",
      "_________________________________________________________________\n",
      "dense1 (Dense)               (64, 1024)                2098176   \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (64, 1024)                0         \n",
      "_________________________________________________________________\n",
      "dense4 (Dense)               (64, 10)                  10250     \n",
      "=================================================================\n",
      "Total params: 2,397,226\n",
      "Trainable params: 2,396,330\n",
      "Non-trainable params: 896\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = keras.Sequential(\n",
    "    [\n",
    "        keras.layers.InputLayer(input_shape=(32, 32,3),batch_size=64),\n",
    "     \n",
    "        keras.layers.Conv2D(32, (3, 3), name=\"conv1\", activation='relu',padding='same'),\n",
    "        keras.layers.BatchNormalization(name=\"bn1\"),  \n",
    "        keras.layers.Conv2D(32, (3, 3),name=\"conv2\", activation='relu', padding='same'),\n",
    "        keras.layers.BatchNormalization(name=\"bn2\"),\n",
    "        keras.layers.MaxPooling2D(pool_size=(2, 2),name=\"mp1\"),\n",
    "        #keras.layers.Dropout(0.2),\n",
    "        keras.layers.Conv2D(64, (3, 3),name=\"conv3\", activation='relu', padding='same'),\n",
    "        keras.layers.BatchNormalization(name=\"bn3\"),\n",
    "        keras.layers.Conv2D(64, (3, 3),name=\"conv4\", activation='relu', padding='same'),\n",
    "        keras.layers.BatchNormalization(name=\"bn4\"),\n",
    "        keras.layers.MaxPooling2D(pool_size=(2, 2),name=\"mp2\"),\n",
    "        #keras.layers.Dropout(0.2),\n",
    "        keras.layers.Conv2D(128, (3, 3),name=\"conv5\", activation='relu', padding='same'),\n",
    "        keras.layers.BatchNormalization(name=\"bn5\"),\n",
    "        keras.layers.Conv2D(128, (3, 3),name=\"conv6\", activation='relu', padding='same'),\n",
    "        keras.layers.BatchNormalization(name=\"bn6\"),\n",
    "        keras.layers.MaxPooling2D(pool_size=(2, 2),name=\"mp3\"),\n",
    "        #keras.layers.Dropout(0.2),\n",
    "     \n",
    "        keras.layers.Flatten(name = \"flatten\"),\n",
    "        keras.layers.Dropout(0.2, name=\"dp1\"),\n",
    "        keras.layers.Dense(1024, name=\"dense1\", activation='relu'),\n",
    "        keras.layers.Dropout(0.2),\n",
    "        #keras.layers.Dense(256, name=\"dense2\", activation='relu'),\n",
    "        #keras.layers.Dropout(0.4),\n",
    "        #keras.layers.Dense(128, name=\"dense3\", activation='relu'),\n",
    "        #keras.layers.BatchNormalization(name=\"bn7\"),\n",
    "        #keras.layers.Dropout(0.4),\n",
    "        keras.layers.Dense(10, name=\"dense4\", activation='softmax')\n",
    "    ],\n",
    "    name=\"modelo-0.0\",\n",
    ")\n",
    "\n",
    "for layer in model.trainable_variables:\n",
    "    #print(layer.name)\n",
    "    if 'bias' in layer.name:\n",
    "        new_bias = tf.cast(tf.where(tf.abs(layer) >= 0, 0.1, 0.1), tf.float32)\n",
    "        layer.assign(new_bias)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50 \t Loss = 1.558 \t Train Acc = 49.060% \t Sparsity = 2.661% \t Test Acc = 52.730%\n",
      "Epoch 2/50 \t Loss = 0.823 \t Train Acc = 53.014% \t Sparsity = 3.287% \t Test Acc = 56.713%\n",
      "Epoch 3/50 \t Loss = 0.638 \t Train Acc = 56.569% \t Sparsity = 3.725% \t Test Acc = 58.608%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-bd12de008a22>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     63\u001b[0m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_batch_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 65\u001b[1;33m       \u001b[0mgrads\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainable_weights\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     66\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mn_bits\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0malpha\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\backprop.py\u001b[0m in \u001b[0;36mgradient\u001b[1;34m(self, target, sources, output_gradients, unconnected_gradients)\u001b[0m\n\u001b[0;32m   1078\u001b[0m                           for x in nest.flatten(output_gradients)]\n\u001b[0;32m   1079\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1080\u001b[1;33m     flat_grad = imperative_grad.imperative_grad(\n\u001b[0m\u001b[0;32m   1081\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_tape\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1082\u001b[0m         \u001b[0mflat_targets\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\imperative_grad.py\u001b[0m in \u001b[0;36mimperative_grad\u001b[1;34m(tape, target, sources, output_gradients, sources_raw, unconnected_gradients)\u001b[0m\n\u001b[0;32m     69\u001b[0m         \"Unknown value for unconnected_gradients: %r\" % unconnected_gradients)\n\u001b[0;32m     70\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 71\u001b[1;33m   return pywrap_tfe.TFE_Py_TapeGradient(\n\u001b[0m\u001b[0;32m     72\u001b[0m       \u001b[0mtape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_tape\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m       \u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\backprop.py\u001b[0m in \u001b[0;36m_gradient_function\u001b[1;34m(op_name, attr_tuple, num_inputs, inputs, outputs, out_grads, skip_input_indices, forward_pass_name_scope)\u001b[0m\n\u001b[0;32m    160\u001b[0m       \u001b[0mgradient_name_scope\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mforward_pass_name_scope\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\"/\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    161\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgradient_name_scope\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 162\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmock_op\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mout_grads\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    163\u001b[0m   \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    164\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmock_op\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mout_grads\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\nn_grad.py\u001b[0m in \u001b[0;36m_FusedBatchNormV3Grad\u001b[1;34m(op, *grad)\u001b[0m\n\u001b[0;32m    935\u001b[0m \u001b[1;33m@\u001b[0m\u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mRegisterGradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"FusedBatchNormV3\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    936\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0m_FusedBatchNormV3Grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 937\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0m_BaseFusedBatchNormGrad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    938\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    939\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\nn_grad.py\u001b[0m in \u001b[0;36m_BaseFusedBatchNormGrad\u001b[1;34m(op, version, *grad)\u001b[0m\n\u001b[0;32m    891\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mversion\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    892\u001b[0m       \u001b[0margs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"reserve_space_3\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 893\u001b[1;33m     \u001b[0mdx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdscale\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdoffset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgrad_fun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    894\u001b[0m   \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    895\u001b[0m     \u001b[0mpop_mean\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\gen_nn_ops.py\u001b[0m in \u001b[0;36mfused_batch_norm_grad_v3\u001b[1;34m(y_backprop, x, scale, reserve_space_1, reserve_space_2, reserve_space_3, epsilon, data_format, is_training, name)\u001b[0m\n\u001b[0;32m   3995\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mtld\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_eager\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3996\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3997\u001b[1;33m       _result = pywrap_tfe.TFE_Py_FastPathExecute(\n\u001b[0m\u001b[0;32m   3998\u001b[0m         \u001b[0m_ctx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"FusedBatchNormGradV3\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_backprop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscale\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3999\u001b[0m         \u001b[0mreserve_space_1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreserve_space_2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreserve_space_3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"epsilon\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs = 50\n",
    "alpha = 0.01\n",
    "n_bits = 32\n",
    "learning_rate = 0.01\n",
    "np.set_printoptions(threshold=np.inf)\n",
    "\n",
    "model_sparsity = np.array([])\n",
    "model_train_loss = np.array([])\n",
    "model_train_acc = np.array([])\n",
    "model_test_loss = np.array([])\n",
    "model_test_acc = np.array([])\n",
    "sparsity = 0\n",
    "\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # Iterate over the batches of the dataset.\n",
    "    loss_batch = np.array([])\n",
    "    loss_test_batch = np.array([])\n",
    "    for step, (x_batch_train, y_batch_train) in enumerate(train_ds):\n",
    "      n_zeros = 0\n",
    "      size = 0\n",
    "      #pruning\n",
    "      if alpha > 0:\n",
    "        bk = []\n",
    "        for layer_weights in model.trainable_variables:\n",
    "          if 'bn' in layer_weights.name:\n",
    "              bk.append(-1)\n",
    "          else:\n",
    "              #flatten weights\n",
    "              f_weights = tf.reshape(layer_weights,[-1])\n",
    "              #get standard deviation of each layer\n",
    "              lim = alpha*tf.math.reduce_std(f_weights)\n",
    "              bk.append(lim)\n",
    "              #create a mask\n",
    "              mask = tf.cast(tf.where(tf.abs(layer_weights)>lim,1,0), tf.float32)\n",
    "              #assign pruned weights to the layer\n",
    "              layer_weights.assign(tf.math.multiply(layer_weights,mask))\n",
    "              #check sparsity\n",
    "              flat_array = np.array((tf.reshape(mask,[-1])))\n",
    "              n_zeros += np.count_nonzero(np.array(flat_array) == 0)\n",
    "              size += flat_array.shape[0]\n",
    "              sparsity = n_zeros*100/size\n",
    "      else:\n",
    "        bk = [0] * len(model.trainable_weights)\n",
    "\n",
    "      #Cópia do modelo\n",
    "      if n_bits > 0 and alpha > 0:\n",
    "          model_copy = keras.models.clone_model(model)\n",
    "          model_copy.set_weights(model.get_weights())\n",
    "      \n",
    "      #Quantização\n",
    "      if n_bits > 0 and alpha > 0:\n",
    "          for i, layer_weights in enumerate(model.trainable_variables):\n",
    "              if 'bn' in layer_weights.name:\n",
    "                  pass\n",
    "              else:\n",
    "                  qk_line = (tf.reduce_max(tf.math.abs(layer_weights)) - bk[i]) / (2 ** (n_bits - 1) - 1)\n",
    "                  ck = tf.math.round(layer_weights / qk_line) * qk_line\n",
    "                  layer_weights.assign(ck)\n",
    "\n",
    "      with tf.GradientTape() as tape:\n",
    "        pred = model(x_batch_train, training=True)\n",
    "        loss = loss_fn(y_batch_train, pred)\n",
    "        \n",
    "      grads = tape.gradient(loss, model.trainable_weights)\n",
    "\n",
    "      if n_bits > 0 and alpha > 0:\n",
    "          for i, (layer_weights, copied_weights) in enumerate(zip(model.trainable_variables, model_copy.trainable_variables)):\n",
    "              grads[i] = grads[i] * learning_rate\n",
    "              # WEIGHT UPDATE\n",
    "              layer_weights.assign(tf.math.subtract(copied_weights, grads[i]))\n",
    "      else:\n",
    "          for i, layer_weights in enumerate(model.trainable_variables):\n",
    "              grads[i] = grads[i] * learning_rate\n",
    "              #WEIGHT UPDATE\n",
    "              layer_weights.assign(tf.math.subtract(layer_weights, grads[i]))\n",
    "          #optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "      \n",
    "      #optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "\n",
    "      predictions = tf.argmax(pred, axis=1, output_type=tf.int32)\n",
    "      acc = train_accuracy(y_batch_train, predictions)\n",
    "\n",
    "      #keep track of loss by batch\n",
    "      loss_batch = np.append(loss_batch, loss)\n",
    "\n",
    "    #mean of loss by epoch\n",
    "    model_train_loss = np.append(model_train_loss, np.mean(loss_batch))\n",
    "    #train accuracy by epoch\n",
    "    model_train_acc = np.append(model_train_acc, acc*100)\n",
    "    #sparsity by epoch\n",
    "    model_sparsity = np.append(model_sparsity, sparsity)\n",
    "\n",
    "    if alpha > 0:\n",
    "        bk = []\n",
    "        for layer_weights in model.trainable_variables:\n",
    "            if 'bn' in layer_weights.name:\n",
    "                bk.append(-1)\n",
    "            else:\n",
    "                #flatten weights\n",
    "                f_weights = tf.reshape(layer_weights,[-1])\n",
    "                #get standard deviation of each layer\n",
    "                lim = alpha*tf.math.reduce_std(f_weights)\n",
    "                bk.append(lim)\n",
    "                #create a mask\n",
    "                mask = tf.cast(tf.where(tf.abs(layer_weights)>lim,1,0), tf.float32)\n",
    "                #assign pruned weights to the layer\n",
    "                layer_weights.assign(tf.math.multiply(layer_weights,mask))\n",
    "                #check sparsity\n",
    "                flat_array = np.array((tf.reshape(mask,[-1])))\n",
    "                n_zeros += np.count_nonzero(np.array(flat_array) == 0)\n",
    "                size += flat_array.shape[0]\n",
    "                sparsity = n_zeros*100/size\n",
    "    else:\n",
    "        bk = [0] * len(model.trainable_weights)\n",
    "\n",
    "    #Quantização\n",
    "    if n_bits > 0 and alpha > 0:\n",
    "        for i, layer_weights in enumerate(model.trainable_variables):\n",
    "            if 'bn' in layer_weights.name:\n",
    "                pass\n",
    "            else:\n",
    "                qk_line = (tf.reduce_max(tf.math.abs(layer_weights)) - bk[i]) / (2 ** (n_bits - 1) - 1)\n",
    "                ck = tf.math.round(layer_weights / qk_line) * qk_line\n",
    "                layer_weights.assign(ck)\n",
    "    \n",
    "    bk.clear()\n",
    "    \n",
    "    #Test\n",
    "    for step, (x_batch_test, y_batch_test) in enumerate(test_ds):\n",
    "      test_pred = model(x_batch_test, training=False)\n",
    "      test_loss = loss_fn(y_batch_test,test_pred)\n",
    "      test_prediction = tf.argmax(test_pred, axis=1, output_type=tf.int32)\n",
    "      test_acc = test_accuracy(y_batch_test, test_prediction)\n",
    "      \n",
    "      loss_test_batch = np.append(loss_batch,test_loss)\n",
    "\n",
    "    model_test_acc = np.append(model_test_acc, test_acc*100)\n",
    "    model_test_loss = np.append(model_test_loss,np.mean(loss_test_batch))\n",
    "    \n",
    "    print(\"Epoch {}/{} \\t Loss = {:.3f} \\t Train Acc = {:.3f}% \\t Sparsity = {:.3f}% \\t Test Acc = {:.3f}%\".format(epoch+1,epochs,float(loss),float(acc*100),sparsity,float(test_acc*100)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = len(model.trainable_weights)\n",
    "for i in range(l):\n",
    "  a = tf.reshape(model.trainable_weights[i],[-1])\n",
    "  b = a.numpy()\n",
    "  #print(a)\n",
    "  #plt.ylim(0,300)\n",
    "  plt.title(str(i))\n",
    "  plt.hist(b,200)\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(\"test acc x train acc\")\n",
    "plt.plot(model_train_acc)\n",
    "plt.plot(model_test_acc)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(\"train loss x test loss\")\n",
    "plt.plot(model_train_loss)\n",
    "plt.plot(model_test_loss)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(\"Sparsity\")\n",
    "plt.plot(model_sparsity)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                        normalize=False,\n",
    "                        title='Confusion matrix',\n",
    "                        cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    figure(figsize=(10, 7), dpi=80)\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(10)\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    #print(cm[:,0])\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, str(cm[i, j]*100/1000) + \"%\",\n",
    "            horizontalalignment=\"center\",\n",
    "            color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('Classe real')\n",
    "    plt.xlabel('Classe predita')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_accuracy = tf.keras.metrics.Accuracy()\n",
    "logits = model(x_test, training=False)\n",
    "prediction = tf.argmax(logits, axis=1, output_type=tf.int32)\n",
    "test_accuracy(prediction, y_test)\n",
    "print(\"Test set accuracy: {:.3%}\".format(test_accuracy.result()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = [\"avião\",\"carro\",\"pássaro\",\"gato\",\"veado\",\"cachorro\",\"sapo\",\"cavalo\",\"navio\",\"caminhão\"]\n",
    "cm = confusion_matrix(y_true=y_test, y_pred=prediction)\n",
    "plot_confusion_matrix(cm=cm, classes=classes, title='Matriz de confusão')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
