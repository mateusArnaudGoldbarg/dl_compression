{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Q8lqzbTc98R",
        "outputId": "387cebbd-5a80-464f-ebf4-973640f8d559"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.7.0\n",
            "1\n",
            "/device:GPU:0\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.pyplot import figure\n",
        "\n",
        "print(tf.__version__)\n",
        "print(len(tf.config.experimental.list_physical_devices('GPU')))\n",
        "device_name = tf.test.gpu_device_name()\n",
        "print(device_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v_J7hEAkc98W",
        "outputId": "98ed88aa-bf6f-4a26-8d79-adb6a1e57126"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170500096/170498071 [==============================] - 2s 0us/step\n",
            "170508288/170498071 [==============================] - 2s 0us/step\n"
          ]
        }
      ],
      "source": [
        "#IMPORTAÇÃO E NORRMALIZAÇÃO\n",
        "(x_train, y_train), (x_test,y_test) = keras.datasets.cifar10.load_data()\n",
        "#x_train = x_train.reshape(-1, 28, 28, 1)\n",
        "#x_test = x_test.reshape(-1, 28, 28, 1)\n",
        "\n",
        "x_train = x_train.astype(float)/255\n",
        "x_test = x_test.astype(float)/255\n",
        "\n",
        "#CRIAR DATASET\n",
        "train_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(50000).batch(64)\n",
        "test_ds = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(64)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "7JY0tvhQc98X"
      },
      "outputs": [],
      "source": [
        "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False)\n",
        "optimizer = tf.keras.optimizers.SGD(learning_rate=0.1)\n",
        "acc_metric = keras.metrics.SparseCategoricalAccuracy()\n",
        "test_accuracy = tf.keras.metrics.Accuracy()\n",
        "test_loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "train_accuracy = tf.keras.metrics.Accuracy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lxIyhmBac98Y",
        "outputId": "e103f353-41b1-4ebc-b0ba-ca88ea457c67"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"modelo-0.0\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv1 (Conv2D)              (64, 32, 32, 32)          896       \n",
            "                                                                 \n",
            " bn1 (BatchNormalization)    (64, 32, 32, 32)          128       \n",
            "                                                                 \n",
            " conv2 (Conv2D)              (64, 32, 32, 32)          9248      \n",
            "                                                                 \n",
            " bn2 (BatchNormalization)    (64, 32, 32, 32)          128       \n",
            "                                                                 \n",
            " mp1 (MaxPooling2D)          (64, 16, 16, 32)          0         \n",
            "                                                                 \n",
            " conv3 (Conv2D)              (64, 16, 16, 64)          18496     \n",
            "                                                                 \n",
            " bn3 (BatchNormalization)    (64, 16, 16, 64)          256       \n",
            "                                                                 \n",
            " conv4 (Conv2D)              (64, 16, 16, 64)          36928     \n",
            "                                                                 \n",
            " bn4 (BatchNormalization)    (64, 16, 16, 64)          256       \n",
            "                                                                 \n",
            " mp2 (MaxPooling2D)          (64, 8, 8, 64)            0         \n",
            "                                                                 \n",
            " conv5 (Conv2D)              (64, 8, 8, 128)           73856     \n",
            "                                                                 \n",
            " bn5 (BatchNormalization)    (64, 8, 8, 128)           512       \n",
            "                                                                 \n",
            " conv6 (Conv2D)              (64, 8, 8, 128)           147584    \n",
            "                                                                 \n",
            " bn6 (BatchNormalization)    (64, 8, 8, 128)           512       \n",
            "                                                                 \n",
            " mp3 (MaxPooling2D)          (64, 4, 4, 128)           0         \n",
            "                                                                 \n",
            " flatten (Flatten)           (64, 2048)                0         \n",
            "                                                                 \n",
            " dp1 (Dropout)               (64, 2048)                0         \n",
            "                                                                 \n",
            " dense1 (Dense)              (64, 1024)                2098176   \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (64, 1024)                0         \n",
            "                                                                 \n",
            " dense4 (Dense)              (64, 10)                  10250     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2,397,226\n",
            "Trainable params: 2,396,330\n",
            "Non-trainable params: 896\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model = keras.Sequential(\n",
        "    [\n",
        "        keras.layers.InputLayer(input_shape=(32, 32,3),batch_size=64),\n",
        "     \n",
        "        keras.layers.Conv2D(32, (3, 3), name=\"conv1\", activation='relu',padding='same'),\n",
        "        keras.layers.BatchNormalization(name=\"bn1\"),  \n",
        "        keras.layers.Conv2D(32, (3, 3),name=\"conv2\", activation='relu', padding='same'),\n",
        "        keras.layers.BatchNormalization(name=\"bn2\"),\n",
        "        keras.layers.MaxPooling2D(pool_size=(2, 2),name=\"mp1\"),\n",
        "        #keras.layers.Dropout(0.2),\n",
        "        keras.layers.Conv2D(64, (3, 3),name=\"conv3\", activation='relu', padding='same'),\n",
        "        keras.layers.BatchNormalization(name=\"bn3\"),\n",
        "        keras.layers.Conv2D(64, (3, 3),name=\"conv4\", activation='relu', padding='same'),\n",
        "        keras.layers.BatchNormalization(name=\"bn4\"),\n",
        "        keras.layers.MaxPooling2D(pool_size=(2, 2),name=\"mp2\"),\n",
        "        #keras.layers.Dropout(0.2),\n",
        "        keras.layers.Conv2D(128, (3, 3),name=\"conv5\", activation='relu', padding='same'),\n",
        "        keras.layers.BatchNormalization(name=\"bn5\"),\n",
        "        keras.layers.Conv2D(128, (3, 3),name=\"conv6\", activation='relu', padding='same'),\n",
        "        keras.layers.BatchNormalization(name=\"bn6\"),\n",
        "        keras.layers.MaxPooling2D(pool_size=(2, 2),name=\"mp3\"),\n",
        "        #keras.layers.Dropout(0.2),\n",
        "     \n",
        "        keras.layers.Flatten(name = \"flatten\"),\n",
        "        keras.layers.Dropout(0.2, name=\"dp1\"),\n",
        "        keras.layers.Dense(1024, name=\"dense1\", activation='relu'),\n",
        "        keras.layers.Dropout(0.2),\n",
        "        #keras.layers.Dense(256, name=\"dense2\", activation='relu'),\n",
        "        #keras.layers.Dropout(0.4),\n",
        "        #keras.layers.Dense(128, name=\"dense3\", activation='relu'),\n",
        "        #keras.layers.BatchNormalization(name=\"bn7\"),\n",
        "        #keras.layers.Dropout(0.4),\n",
        "        keras.layers.Dense(10, name=\"dense4\", activation='softmax')\n",
        "    ],\n",
        "    name=\"modelo-0.0\",\n",
        ")\n",
        "\n",
        "for layer in model.trainable_variables:\n",
        "    #print(layer.name)\n",
        "    if 'bias' in layer.name:\n",
        "        new_bias = tf.cast(tf.where(tf.abs(layer) >= 0, 0.1, 0.1), tf.float32)\n",
        "        layer.assign(new_bias)\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lNxvzk3pc98Z",
        "outputId": "ad823622-926d-4976-b25d-31e6484382d7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<string>:6: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100 \t Loss = 0.711 \t Train Acc = 53.096% \t Sparsity = 39.040% \t Test Acc = 63.540%\n",
            "Epoch 2/100 \t Loss = 0.695 \t Train Acc = 61.117% \t Sparsity = 42.030% \t Test Acc = 64.795%\n",
            "Epoch 3/100 \t Loss = 0.545 \t Train Acc = 65.919% \t Sparsity = 44.302% \t Test Acc = 68.107%\n",
            "Epoch 4/100 \t Loss = 1.081 \t Train Acc = 69.279% \t Sparsity = 46.268% \t Test Acc = 70.213%\n",
            "Epoch 5/100 \t Loss = 0.352 \t Train Acc = 71.905% \t Sparsity = 48.050% \t Test Acc = 71.830%\n",
            "Epoch 6/100 \t Loss = 0.542 \t Train Acc = 74.107% \t Sparsity = 49.719% \t Test Acc = 73.067%\n",
            "Epoch 7/100 \t Loss = 0.406 \t Train Acc = 75.984% \t Sparsity = 51.285% \t Test Acc = 73.983%\n",
            "Epoch 8/100 \t Loss = 0.233 \t Train Acc = 77.627% \t Sparsity = 52.762% \t Test Acc = 74.677%\n",
            "Epoch 9/100 \t Loss = 0.207 \t Train Acc = 79.056% \t Sparsity = 54.180% \t Test Acc = 75.310%\n",
            "Epoch 10/100 \t Loss = 0.292 \t Train Acc = 80.326% \t Sparsity = 55.449% \t Test Acc = 75.818%\n",
            "Epoch 11/100 \t Loss = 0.236 \t Train Acc = 81.497% \t Sparsity = 56.575% \t Test Acc = 76.240%\n",
            "Epoch 12/100 \t Loss = 0.119 \t Train Acc = 82.534% \t Sparsity = 57.631% \t Test Acc = 76.601%\n",
            "Epoch 13/100 \t Loss = 0.336 \t Train Acc = 83.463% \t Sparsity = 58.597% \t Test Acc = 76.905%\n",
            "Epoch 14/100 \t Loss = 0.215 \t Train Acc = 84.289% \t Sparsity = 59.500% \t Test Acc = 77.195%\n",
            "Epoch 15/100 \t Loss = 0.032 \t Train Acc = 85.051% \t Sparsity = 60.284% \t Test Acc = 77.433%\n",
            "Epoch 16/100 \t Loss = 0.133 \t Train Acc = 85.735% \t Sparsity = 60.996% \t Test Acc = 77.647%\n",
            "Epoch 17/100 \t Loss = 0.024 \t Train Acc = 86.368% \t Sparsity = 61.654% \t Test Acc = 77.862%\n",
            "Epoch 18/100 \t Loss = 0.194 \t Train Acc = 86.938% \t Sparsity = 62.271% \t Test Acc = 78.019%\n",
            "Epoch 19/100 \t Loss = 0.047 \t Train Acc = 87.458% \t Sparsity = 62.841% \t Test Acc = 78.191%\n",
            "Epoch 20/100 \t Loss = 0.400 \t Train Acc = 87.939% \t Sparsity = 63.388% \t Test Acc = 78.316%\n",
            "Epoch 21/100 \t Loss = 0.119 \t Train Acc = 88.382% \t Sparsity = 63.895% \t Test Acc = 78.451%\n",
            "Epoch 22/100 \t Loss = 0.034 \t Train Acc = 88.791% \t Sparsity = 64.363% \t Test Acc = 78.600%\n",
            "Epoch 23/100 \t Loss = 0.117 \t Train Acc = 89.170% \t Sparsity = 64.808% \t Test Acc = 78.750%\n",
            "Epoch 24/100 \t Loss = 0.168 \t Train Acc = 89.523% \t Sparsity = 65.228% \t Test Acc = 78.866%\n",
            "Epoch 25/100 \t Loss = 0.452 \t Train Acc = 89.856% \t Sparsity = 65.595% \t Test Acc = 78.967%\n",
            "Epoch 26/100 \t Loss = 0.155 \t Train Acc = 90.165% \t Sparsity = 65.975% \t Test Acc = 79.074%\n",
            "Epoch 27/100 \t Loss = 0.196 \t Train Acc = 90.453% \t Sparsity = 66.327% \t Test Acc = 79.170%\n",
            "Epoch 28/100 \t Loss = 0.002 \t Train Acc = 90.722% \t Sparsity = 66.662% \t Test Acc = 79.264%\n",
            "Epoch 29/100 \t Loss = 0.004 \t Train Acc = 90.978% \t Sparsity = 66.977% \t Test Acc = 79.360%\n",
            "Epoch 30/100 \t Loss = 0.008 \t Train Acc = 91.222% \t Sparsity = 67.261% \t Test Acc = 79.427%\n",
            "Epoch 31/100 \t Loss = 0.014 \t Train Acc = 91.451% \t Sparsity = 67.530% \t Test Acc = 79.500%\n",
            "Epoch 32/100 \t Loss = 0.003 \t Train Acc = 91.669% \t Sparsity = 67.790% \t Test Acc = 79.573%\n",
            "Epoch 33/100 \t Loss = 0.004 \t Train Acc = 91.875% \t Sparsity = 68.031% \t Test Acc = 79.643%\n",
            "Epoch 34/100 \t Loss = 0.189 \t Train Acc = 92.071% \t Sparsity = 68.265% \t Test Acc = 79.707%\n",
            "Epoch 35/100 \t Loss = 0.024 \t Train Acc = 92.254% \t Sparsity = 68.515% \t Test Acc = 79.774%\n",
            "Epoch 36/100 \t Loss = 0.260 \t Train Acc = 92.433% \t Sparsity = 68.726% \t Test Acc = 79.839%\n",
            "Epoch 37/100 \t Loss = 0.003 \t Train Acc = 92.599% \t Sparsity = 68.943% \t Test Acc = 79.902%\n",
            "Epoch 38/100 \t Loss = 0.247 \t Train Acc = 92.760% \t Sparsity = 69.131% \t Test Acc = 79.955%\n",
            "Epoch 39/100 \t Loss = 0.001 \t Train Acc = 92.911% \t Sparsity = 69.339% \t Test Acc = 80.013%\n",
            "Epoch 40/100 \t Loss = 0.012 \t Train Acc = 93.057% \t Sparsity = 69.542% \t Test Acc = 80.067%\n",
            "Epoch 41/100 \t Loss = 0.022 \t Train Acc = 93.192% \t Sparsity = 69.750% \t Test Acc = 80.118%\n",
            "Epoch 42/100 \t Loss = 0.006 \t Train Acc = 93.324% \t Sparsity = 69.939% \t Test Acc = 80.157%\n",
            "Epoch 43/100 \t Loss = 0.092 \t Train Acc = 93.451% \t Sparsity = 70.120% \t Test Acc = 80.210%\n",
            "Epoch 44/100 \t Loss = 0.003 \t Train Acc = 93.573% \t Sparsity = 70.290% \t Test Acc = 80.256%\n",
            "Epoch 45/100 \t Loss = 0.000 \t Train Acc = 93.691% \t Sparsity = 70.460% \t Test Acc = 80.301%\n",
            "Epoch 46/100 \t Loss = 0.014 \t Train Acc = 93.804% \t Sparsity = 70.617% \t Test Acc = 80.343%\n",
            "Epoch 47/100 \t Loss = 0.088 \t Train Acc = 93.913% \t Sparsity = 70.763% \t Test Acc = 80.388%\n",
            "Epoch 48/100 \t Loss = 0.072 \t Train Acc = 94.019% \t Sparsity = 70.908% \t Test Acc = 80.437%\n",
            "Epoch 49/100 \t Loss = 0.000 \t Train Acc = 94.118% \t Sparsity = 71.060% \t Test Acc = 80.476%\n",
            "Epoch 50/100 \t Loss = 0.121 \t Train Acc = 94.215% \t Sparsity = 71.193% \t Test Acc = 80.519%\n",
            "Epoch 51/100 \t Loss = 0.186 \t Train Acc = 94.309% \t Sparsity = 71.331% \t Test Acc = 80.556%\n",
            "Epoch 52/100 \t Loss = 0.003 \t Train Acc = 94.401% \t Sparsity = 71.463% \t Test Acc = 80.585%\n",
            "Epoch 53/100 \t Loss = 0.017 \t Train Acc = 94.487% \t Sparsity = 71.598% \t Test Acc = 80.614%\n",
            "Epoch 54/100 \t Loss = 0.004 \t Train Acc = 94.570% \t Sparsity = 71.731% \t Test Acc = 80.650%\n",
            "Epoch 55/100 \t Loss = 0.004 \t Train Acc = 94.651% \t Sparsity = 71.869% \t Test Acc = 80.679%\n",
            "Epoch 56/100 \t Loss = 0.027 \t Train Acc = 94.728% \t Sparsity = 71.997% \t Test Acc = 80.711%\n",
            "Epoch 57/100 \t Loss = 0.000 \t Train Acc = 94.802% \t Sparsity = 72.128% \t Test Acc = 80.739%\n",
            "Epoch 58/100 \t Loss = 0.007 \t Train Acc = 94.877% \t Sparsity = 72.237% \t Test Acc = 80.774%\n",
            "Epoch 59/100 \t Loss = 0.148 \t Train Acc = 94.950% \t Sparsity = 72.343% \t Test Acc = 80.808%\n",
            "Epoch 60/100 \t Loss = 0.007 \t Train Acc = 95.020% \t Sparsity = 72.452% \t Test Acc = 80.837%\n",
            "Epoch 61/100 \t Loss = 0.014 \t Train Acc = 95.089% \t Sparsity = 72.549% \t Test Acc = 80.859%\n",
            "Epoch 62/100 \t Loss = 0.002 \t Train Acc = 95.155% \t Sparsity = 72.653% \t Test Acc = 80.876%\n",
            "Epoch 63/100 \t Loss = 0.018 \t Train Acc = 95.219% \t Sparsity = 72.758% \t Test Acc = 80.903%\n",
            "Epoch 64/100 \t Loss = 0.032 \t Train Acc = 95.281% \t Sparsity = 72.859% \t Test Acc = 80.931%\n",
            "Epoch 65/100 \t Loss = 0.121 \t Train Acc = 95.341% \t Sparsity = 72.954% \t Test Acc = 80.953%\n",
            "Epoch 66/100 \t Loss = 0.085 \t Train Acc = 95.401% \t Sparsity = 73.047% \t Test Acc = 80.983%\n",
            "Epoch 67/100 \t Loss = 0.105 \t Train Acc = 95.457% \t Sparsity = 73.145% \t Test Acc = 80.998%\n",
            "Epoch 68/100 \t Loss = 0.004 \t Train Acc = 95.512% \t Sparsity = 73.245% \t Test Acc = 81.024%\n",
            "Epoch 69/100 \t Loss = 0.000 \t Train Acc = 95.565% \t Sparsity = 73.345% \t Test Acc = 81.048%\n",
            "Epoch 70/100 \t Loss = 0.007 \t Train Acc = 95.617% \t Sparsity = 73.435% \t Test Acc = 81.068%\n",
            "Epoch 71/100 \t Loss = 0.151 \t Train Acc = 95.669% \t Sparsity = 73.519% \t Test Acc = 81.082%\n",
            "Epoch 72/100 \t Loss = 0.019 \t Train Acc = 95.718% \t Sparsity = 73.609% \t Test Acc = 81.103%\n",
            "Epoch 73/100 \t Loss = 0.000 \t Train Acc = 95.768% \t Sparsity = 73.684% \t Test Acc = 81.127%\n",
            "Epoch 74/100 \t Loss = 0.002 \t Train Acc = 95.815% \t Sparsity = 73.768% \t Test Acc = 81.148%\n",
            "Epoch 75/100 \t Loss = 0.031 \t Train Acc = 95.860% \t Sparsity = 73.857% \t Test Acc = 81.161%\n",
            "Epoch 76/100 \t Loss = 0.014 \t Train Acc = 95.906% \t Sparsity = 73.938% \t Test Acc = 81.182%\n",
            "Epoch 77/100 \t Loss = 0.002 \t Train Acc = 95.950% \t Sparsity = 74.013% \t Test Acc = 81.197%\n",
            "Epoch 78/100 \t Loss = 0.000 \t Train Acc = 95.993% \t Sparsity = 74.089% \t Test Acc = 81.209%\n",
            "Epoch 79/100 \t Loss = 0.332 \t Train Acc = 96.035% \t Sparsity = 74.162% \t Test Acc = 81.226%\n",
            "Epoch 80/100 \t Loss = 0.003 \t Train Acc = 96.076% \t Sparsity = 74.247% \t Test Acc = 81.240%\n"
          ]
        }
      ],
      "source": [
        "epochs = 100\n",
        "alpha = 0.5\n",
        "n_bits = 32\n",
        "\n",
        "learning_rate = 0.01\n",
        "momentum = 0.9\n",
        "\n",
        "np.set_printoptions(threshold=np.inf)\n",
        "\n",
        "model_sparsity = np.array([])\n",
        "model_train_loss = np.array([])\n",
        "model_train_acc = np.array([])\n",
        "model_test_loss = np.array([])\n",
        "model_test_acc = np.array([])\n",
        "sparsity = 0\n",
        "#TODO: Increase batch_size and just quantize batch_norm layer\n",
        "with tf.device('/device:GPU:0'):\n",
        "    for epoch in range(epochs):\n",
        "        # Iterate over the batches of the dataset.\n",
        "        loss_batch = np.array([])\n",
        "        loss_test_batch = np.array([])\n",
        "        for step, (x_batch_train, y_batch_train) in enumerate(train_ds):\n",
        "          n_zeros = 0\n",
        "          size = 0\n",
        "          #pruning\n",
        "          if alpha > 0:\n",
        "            bk = []\n",
        "            for layer_weights in model.trainable_variables:\n",
        "              if 'bn' in layer_weights.name:\n",
        "                  bk.append(-1)\n",
        "              else:\n",
        "                  #flatten weights\n",
        "                  f_weights = tf.reshape(layer_weights,[-1])\n",
        "                  #get standard deviation of each layer\n",
        "                  lim = alpha*tf.math.reduce_std(f_weights)\n",
        "                  bk.append(lim)\n",
        "                  #create a mask\n",
        "                  mask = tf.cast(tf.where(tf.abs(layer_weights)>lim,1,0), tf.float32)\n",
        "                  #assign pruned weights to the layer\n",
        "                  layer_weights.assign(tf.math.multiply(layer_weights,mask))\n",
        "                  #check sparsity\n",
        "                  flat_array = np.array((tf.reshape(mask,[-1])))\n",
        "                  n_zeros += np.count_nonzero(np.array(flat_array) == 0)\n",
        "                  size += flat_array.shape[0]\n",
        "                  sparsity = n_zeros*100/size\n",
        "          else:\n",
        "            bk = [0] * len(model.trainable_weights)\n",
        "\n",
        "          #Cópia do modelo\n",
        "          if n_bits > 0 and alpha > 0:\n",
        "              model_copy = keras.models.clone_model(model)\n",
        "              model_copy.set_weights(model.get_weights())\n",
        "          \n",
        "          #Quantização\n",
        "          if n_bits > 0 and alpha > 0:\n",
        "              for i, layer_weights in enumerate(model.trainable_variables):\n",
        "                  if 'bn' in layer_weights.name:\n",
        "                      pass\n",
        "                  else:\n",
        "                      qk_line = (tf.reduce_max(tf.math.abs(layer_weights)) - bk[i]) / (2 ** (n_bits - 1) - 1)\n",
        "                      ck = tf.math.round(layer_weights / qk_line) * qk_line\n",
        "                      layer_weights.assign(ck)\n",
        "\n",
        "          with tf.GradientTape() as tape:\n",
        "            pred = model(x_batch_train, training=True)\n",
        "            loss = loss_fn(y_batch_train, pred)\n",
        "            \n",
        "          grads = tape.gradient(loss, model.trainable_weights)\n",
        "          if step==0 and epoch==0:\n",
        "              v = np.zeros_like(grads)\n",
        "          if n_bits > 0 and alpha > 0:\n",
        "              for i, (layer_weights, copied_weights) in enumerate(zip(model.trainable_variables, model_copy.trainable_variables)):\n",
        "                  #TODO: Add momentum and velocity\n",
        "                  grads[i] = grads[i] * learning_rate\n",
        "                  #Get value of velocity\n",
        "                  v[i] = tf.math.subtract(momentum*v[i], grads[i])\n",
        "                  # WEIGHT UPDATE\n",
        "                  layer_weights.assign(tf.math.add(copied_weights, v[i]))\n",
        "          else:\n",
        "              for i, layer_weights in enumerate(model.trainable_variables):\n",
        "                  #TODO: Include momentum here\n",
        "                  grads[i] = grads[i] * learning_rate\n",
        "                  #WEIGHT UPDATE\n",
        "                  layer_weights.assign(tf.math.subtract(layer_weights, grads[i]))\n",
        "              #optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
        "          \n",
        "          #optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
        "\n",
        "          predictions = tf.argmax(pred, axis=1, output_type=tf.int32)\n",
        "          acc = train_accuracy(y_batch_train, predictions)\n",
        "\n",
        "          #keep track of loss by batch\n",
        "          loss_batch = np.append(loss_batch, loss)\n",
        "\n",
        "        #mean of loss by epoch\n",
        "        model_train_loss = np.append(model_train_loss, np.mean(loss_batch))\n",
        "        #train accuracy by epoch\n",
        "        model_train_acc = np.append(model_train_acc, acc*100)\n",
        "        #sparsity by epoch\n",
        "        model_sparsity = np.append(model_sparsity, sparsity)\n",
        "\n",
        "        if alpha > 0:\n",
        "            bk = []\n",
        "            for layer_weights in model.trainable_variables:\n",
        "                if 'bn' in layer_weights.name:\n",
        "                    bk.append(-1)\n",
        "                else:\n",
        "                    #flatten weights\n",
        "                    f_weights = tf.reshape(layer_weights,[-1])\n",
        "                    #get standard deviation of each layer\n",
        "                    lim = alpha*tf.math.reduce_std(f_weights)\n",
        "                    bk.append(lim)\n",
        "                    #create a mask\n",
        "                    mask = tf.cast(tf.where(tf.abs(layer_weights)>lim,1,0), tf.float32)\n",
        "                    #assign pruned weights to the layer\n",
        "                    layer_weights.assign(tf.math.multiply(layer_weights,mask))\n",
        "                    #check sparsity\n",
        "                    flat_array = np.array((tf.reshape(mask,[-1])))\n",
        "                    n_zeros += np.count_nonzero(np.array(flat_array) == 0)\n",
        "                    size += flat_array.shape[0]\n",
        "                    sparsity = n_zeros*100/size\n",
        "        else:\n",
        "            bk = [0] * len(model.trainable_weights)\n",
        "\n",
        "        #Quantização\n",
        "        if n_bits > 0 and alpha > 0:\n",
        "            for i, layer_weights in enumerate(model.trainable_variables):\n",
        "                if 'bn' in layer_weights.name:\n",
        "                    pass\n",
        "                else:\n",
        "                    qk_line = (tf.reduce_max(tf.math.abs(layer_weights)) - bk[i]) / (2 ** (n_bits - 1) - 1)\n",
        "                    ck = tf.math.round(layer_weights / qk_line) * qk_line\n",
        "                    layer_weights.assign(ck)\n",
        "        \n",
        "        bk.clear()\n",
        "        \n",
        "        #Test\n",
        "        for step, (x_batch_test, y_batch_test) in enumerate(test_ds):\n",
        "          test_pred = model(x_batch_test, training=False)\n",
        "          test_loss = loss_fn(y_batch_test,test_pred)\n",
        "          test_prediction = tf.argmax(test_pred, axis=1, output_type=tf.int32)\n",
        "          test_acc = test_accuracy(y_batch_test, test_prediction)\n",
        "          \n",
        "          loss_test_batch = np.append(loss_batch,test_loss)\n",
        "\n",
        "        model_test_acc = np.append(model_test_acc, test_acc*100)\n",
        "        model_test_loss = np.append(model_test_loss,np.mean(loss_test_batch))\n",
        "        \n",
        "        print(\"Epoch {}/{} \\t Loss = {:.3f} \\t Train Acc = {:.3f}% \\t Sparsity = {:.3f}% \\t Test Acc = {:.3f}%\".format(epoch+1,epochs,float(loss),float(acc*100),sparsity,float(test_acc*100)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qH9TwrMnc98a"
      },
      "outputs": [],
      "source": [
        "l = len(model.trainable_weights)\n",
        "for i in range(l):\n",
        "  a = tf.reshape(model.trainable_weights[i],[-1])\n",
        "  b = a.numpy()\n",
        "  #print(a)\n",
        "  #plt.ylim(0,300)\n",
        "  plt.title(str(i))\n",
        "  plt.hist(b,200)\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5eo_wg9tc98b"
      },
      "outputs": [],
      "source": [
        "plt.title(\"test acc x train acc\")\n",
        "plt.plot(model_train_acc)\n",
        "plt.plot(model_test_acc)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RaQc5JNFc98c"
      },
      "outputs": [],
      "source": [
        "plt.title(\"train loss x test loss\")\n",
        "plt.plot(model_train_loss)\n",
        "plt.plot(model_test_loss)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ioeGU-jjc98c"
      },
      "outputs": [],
      "source": [
        "plt.title(\"Sparsity\")\n",
        "plt.plot(model_sparsity)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oSh5m9tTc98d"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import itertools\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_confusion_matrix(cm, classes,\n",
        "                        normalize=False,\n",
        "                        title='Confusion matrix',\n",
        "                        cmap=plt.cm.Blues):\n",
        "    \"\"\"\n",
        "    This function prints and plots the confusion matrix.\n",
        "    Normalization can be applied by setting `normalize=True`.\n",
        "    \"\"\"\n",
        "    figure(figsize=(10, 7), dpi=80)\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(title)\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(10)\n",
        "    plt.xticks(tick_marks, classes, rotation=45)\n",
        "    plt.yticks(tick_marks, classes)\n",
        "\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "        print(\"Normalized confusion matrix\")\n",
        "    else:\n",
        "        print('Confusion matrix, without normalization')\n",
        "\n",
        "    #print(cm[:,0])\n",
        "\n",
        "    thresh = cm.max() / 2.\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        plt.text(j, i, str(cm[i, j]*100/1000) + \"%\",\n",
        "            horizontalalignment=\"center\",\n",
        "            color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.ylabel('Classe real')\n",
        "    plt.xlabel('Classe predita')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qwd39wm-c98e"
      },
      "outputs": [],
      "source": [
        "test_accuracy = tf.keras.metrics.Accuracy()\n",
        "logits = model(x_test, training=False)\n",
        "prediction = tf.argmax(logits, axis=1, output_type=tf.int32)\n",
        "test_accuracy(prediction, y_test)\n",
        "print(\"Test set accuracy: {:.3%}\".format(test_accuracy.result()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BeB7WM_Xc98f"
      },
      "outputs": [],
      "source": [
        "classes = [\"avião\",\"carro\",\"pássaro\",\"gato\",\"veado\",\"cachorro\",\"sapo\",\"cavalo\",\"navio\",\"caminhão\"]\n",
        "cm = confusion_matrix(y_true=y_test, y_pred=prediction)\n",
        "plot_confusion_matrix(cm=cm, classes=classes, title='Matriz de confusão')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "SkAsvvxbhb9S"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "dl_newModel.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}