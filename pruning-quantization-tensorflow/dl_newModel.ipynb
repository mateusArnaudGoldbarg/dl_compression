{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Q8lqzbTc98R",
        "outputId": "62a25ba7-b404-49e3-9876-caeef19bdc47"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.7.0\n",
            "1\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.pyplot import figure\n",
        "\n",
        "print(tf.__version__)\n",
        "print(len(tf.config.experimental.list_physical_devices('GPU')))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v_J7hEAkc98W",
        "outputId": "e6d3be70-bc8c-45df-b876-6dbe27e64530"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170500096/170498071 [==============================] - 11s 0us/step\n",
            "170508288/170498071 [==============================] - 11s 0us/step\n"
          ]
        }
      ],
      "source": [
        "#IMPORTAÇÃO E NORRMALIZAÇÃO\n",
        "(x_train, y_train), (x_test,y_test) = keras.datasets.cifar10.load_data()\n",
        "#x_train = x_train.reshape(-1, 28, 28, 1)\n",
        "#x_test = x_test.reshape(-1, 28, 28, 1)\n",
        "\n",
        "x_train = x_train.astype(float)/255\n",
        "x_test = x_test.astype(float)/255\n",
        "\n",
        "#CRIAR DATASET\n",
        "train_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(50000).batch(64)\n",
        "test_ds = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(64)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7JY0tvhQc98X"
      },
      "outputs": [],
      "source": [
        "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False)\n",
        "optimizer = tf.keras.optimizers.SGD(learning_rate=0.1)\n",
        "acc_metric = keras.metrics.SparseCategoricalAccuracy()\n",
        "test_accuracy = tf.keras.metrics.Accuracy()\n",
        "test_loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "train_accuracy = tf.keras.metrics.Accuracy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lxIyhmBac98Y",
        "outputId": "bbffaf38-f59d-4533-e524-972f13bf360d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"modelo-0.0\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv1 (Conv2D)              (64, 32, 32, 32)          896       \n",
            "                                                                 \n",
            " bn1 (BatchNormalization)    (64, 32, 32, 32)          128       \n",
            "                                                                 \n",
            " conv2 (Conv2D)              (64, 32, 32, 32)          9248      \n",
            "                                                                 \n",
            " bn2 (BatchNormalization)    (64, 32, 32, 32)          128       \n",
            "                                                                 \n",
            " mp1 (MaxPooling2D)          (64, 16, 16, 32)          0         \n",
            "                                                                 \n",
            " conv3 (Conv2D)              (64, 16, 16, 64)          18496     \n",
            "                                                                 \n",
            " bn3 (BatchNormalization)    (64, 16, 16, 64)          256       \n",
            "                                                                 \n",
            " conv4 (Conv2D)              (64, 16, 16, 64)          36928     \n",
            "                                                                 \n",
            " bn4 (BatchNormalization)    (64, 16, 16, 64)          256       \n",
            "                                                                 \n",
            " mp2 (MaxPooling2D)          (64, 8, 8, 64)            0         \n",
            "                                                                 \n",
            " conv5 (Conv2D)              (64, 8, 8, 128)           73856     \n",
            "                                                                 \n",
            " bn5 (BatchNormalization)    (64, 8, 8, 128)           512       \n",
            "                                                                 \n",
            " conv6 (Conv2D)              (64, 8, 8, 128)           147584    \n",
            "                                                                 \n",
            " bn6 (BatchNormalization)    (64, 8, 8, 128)           512       \n",
            "                                                                 \n",
            " mp3 (MaxPooling2D)          (64, 4, 4, 128)           0         \n",
            "                                                                 \n",
            " flatten (Flatten)           (64, 2048)                0         \n",
            "                                                                 \n",
            " dp1 (Dropout)               (64, 2048)                0         \n",
            "                                                                 \n",
            " dense1 (Dense)              (64, 1024)                2098176   \n",
            "                                                                 \n",
            " dropout (Dropout)           (64, 1024)                0         \n",
            "                                                                 \n",
            " dense4 (Dense)              (64, 10)                  10250     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2,397,226\n",
            "Trainable params: 2,396,330\n",
            "Non-trainable params: 896\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model = keras.Sequential(\n",
        "    [\n",
        "        keras.layers.InputLayer(input_shape=(32, 32,3),batch_size=64),\n",
        "     \n",
        "        keras.layers.Conv2D(32, (3, 3), name=\"conv1\", activation='relu',padding='same'),\n",
        "        keras.layers.BatchNormalization(name=\"bn1\"),  \n",
        "        keras.layers.Conv2D(32, (3, 3),name=\"conv2\", activation='relu', padding='same'),\n",
        "        keras.layers.BatchNormalization(name=\"bn2\"),\n",
        "        keras.layers.MaxPooling2D(pool_size=(2, 2),name=\"mp1\"),\n",
        "        #keras.layers.Dropout(0.2),\n",
        "        keras.layers.Conv2D(64, (3, 3),name=\"conv3\", activation='relu', padding='same'),\n",
        "        keras.layers.BatchNormalization(name=\"bn3\"),\n",
        "        keras.layers.Conv2D(64, (3, 3),name=\"conv4\", activation='relu', padding='same'),\n",
        "        keras.layers.BatchNormalization(name=\"bn4\"),\n",
        "        keras.layers.MaxPooling2D(pool_size=(2, 2),name=\"mp2\"),\n",
        "        #keras.layers.Dropout(0.2),\n",
        "        keras.layers.Conv2D(128, (3, 3),name=\"conv5\", activation='relu', padding='same'),\n",
        "        keras.layers.BatchNormalization(name=\"bn5\"),\n",
        "        keras.layers.Conv2D(128, (3, 3),name=\"conv6\", activation='relu', padding='same'),\n",
        "        keras.layers.BatchNormalization(name=\"bn6\"),\n",
        "        keras.layers.MaxPooling2D(pool_size=(2, 2),name=\"mp3\"),\n",
        "        #keras.layers.Dropout(0.2),\n",
        "     \n",
        "        keras.layers.Flatten(name = \"flatten\"),\n",
        "        keras.layers.Dropout(0.2, name=\"dp1\"),\n",
        "        keras.layers.Dense(1024, name=\"dense1\", activation='relu'),\n",
        "        keras.layers.Dropout(0.2),\n",
        "        #keras.layers.Dense(256, name=\"dense2\", activation='relu'),\n",
        "        #keras.layers.Dropout(0.4),\n",
        "        #keras.layers.Dense(128, name=\"dense3\", activation='relu'),\n",
        "        #keras.layers.BatchNormalization(name=\"bn7\"),\n",
        "        #keras.layers.Dropout(0.4),\n",
        "        keras.layers.Dense(10, name=\"dense4\", activation='softmax')\n",
        "    ],\n",
        "    name=\"modelo-0.0\",\n",
        ")\n",
        "\n",
        "for layer in model.trainable_variables:\n",
        "    #print(layer.name)\n",
        "    if 'bias' in layer.name:\n",
        "        new_bias = tf.cast(tf.where(tf.abs(layer) >= 0, 0.1, 0.1), tf.float32)\n",
        "        layer.assign(new_bias)\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lNxvzk3pc98Z",
        "outputId": "941d27f0-8f23-4f9c-aa81-07f640650ce4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50 \t Loss = 1.266 \t Train Acc = 47.708% \t Sparsity = 2.674% \t Test Acc = 54.520%\n",
            "Epoch 2/50 \t Loss = 0.798 \t Train Acc = 55.510% \t Sparsity = 3.320% \t Test Acc = 59.035%\n",
            "Epoch 3/50 \t Loss = 0.757 \t Train Acc = 60.227% \t Sparsity = 3.751% \t Test Acc = 62.723%\n",
            "Epoch 4/50 \t Loss = 0.368 \t Train Acc = 63.459% \t Sparsity = 4.120% \t Test Acc = 64.707%\n",
            "Epoch 5/50 \t Loss = 0.593 \t Train Acc = 65.964% \t Sparsity = 4.411% \t Test Acc = 66.530%\n",
            "Epoch 6/50 \t Loss = 0.709 \t Train Acc = 68.023% \t Sparsity = 4.674% \t Test Acc = 67.340%\n",
            "Epoch 7/50 \t Loss = 0.528 \t Train Acc = 69.719% \t Sparsity = 4.943% \t Test Acc = 68.139%\n",
            "Epoch 8/50 \t Loss = 0.676 \t Train Acc = 71.188% \t Sparsity = 5.162% \t Test Acc = 69.079%\n",
            "Epoch 9/50 \t Loss = 0.555 \t Train Acc = 72.511% \t Sparsity = 5.436% \t Test Acc = 69.639%\n",
            "Epoch 10/50 \t Loss = 0.409 \t Train Acc = 73.725% \t Sparsity = 5.680% \t Test Acc = 70.383%\n",
            "Epoch 11/50 \t Loss = 0.572 \t Train Acc = 74.832% \t Sparsity = 5.825% \t Test Acc = 70.962%\n",
            "Epoch 12/50 \t Loss = 0.886 \t Train Acc = 75.824% \t Sparsity = 5.922% \t Test Acc = 71.499%\n",
            "Epoch 13/50 \t Loss = 0.186 \t Train Acc = 76.764% \t Sparsity = 6.283% \t Test Acc = 72.065%\n",
            "Epoch 14/50 \t Loss = 0.477 \t Train Acc = 77.640% \t Sparsity = 6.317% \t Test Acc = 72.436%\n",
            "Epoch 15/50 \t Loss = 0.161 \t Train Acc = 78.454% \t Sparsity = 6.602% \t Test Acc = 72.857%\n",
            "Epoch 16/50 \t Loss = 0.213 \t Train Acc = 79.231% \t Sparsity = 6.759% \t Test Acc = 73.173%\n",
            "Epoch 17/50 \t Loss = 0.222 \t Train Acc = 79.953% \t Sparsity = 6.873% \t Test Acc = 73.473%\n",
            "Epoch 18/50 \t Loss = 0.375 \t Train Acc = 80.636% \t Sparsity = 6.997% \t Test Acc = 73.722%\n",
            "Epoch 19/50 \t Loss = 0.169 \t Train Acc = 81.288% \t Sparsity = 7.160% \t Test Acc = 73.984%\n",
            "Epoch 20/50 \t Loss = 0.529 \t Train Acc = 81.899% \t Sparsity = 7.153% \t Test Acc = 74.171%\n",
            "Epoch 21/50 \t Loss = 0.104 \t Train Acc = 82.485% \t Sparsity = 7.473% \t Test Acc = 74.391%\n",
            "Epoch 22/50 \t Loss = 0.376 \t Train Acc = 83.047% \t Sparsity = 7.408% \t Test Acc = 74.573%\n",
            "Epoch 23/50 \t Loss = 0.230 \t Train Acc = 83.564% \t Sparsity = 7.568% \t Test Acc = 74.757%\n",
            "Epoch 24/50 \t Loss = 0.140 \t Train Acc = 84.061% \t Sparsity = 7.741% \t Test Acc = 74.888%\n",
            "Epoch 25/50 \t Loss = 0.183 \t Train Acc = 84.522% \t Sparsity = 7.748% \t Test Acc = 75.033%\n",
            "Epoch 26/50 \t Loss = 0.051 \t Train Acc = 84.969% \t Sparsity = 7.992% \t Test Acc = 75.224%\n",
            "Epoch 27/50 \t Loss = 0.037 \t Train Acc = 85.387% \t Sparsity = 8.068% \t Test Acc = 75.402%\n",
            "Epoch 28/50 \t Loss = 0.205 \t Train Acc = 85.789% \t Sparsity = 7.993% \t Test Acc = 75.508%\n",
            "Epoch 29/50 \t Loss = 0.018 \t Train Acc = 86.173% \t Sparsity = 8.258% \t Test Acc = 75.642%\n",
            "Epoch 30/50 \t Loss = 0.163 \t Train Acc = 86.535% \t Sparsity = 8.206% \t Test Acc = 75.765%\n",
            "Epoch 31/50 \t Loss = 0.138 \t Train Acc = 86.882% \t Sparsity = 8.299% \t Test Acc = 75.842%\n",
            "Epoch 32/50 \t Loss = 0.061 \t Train Acc = 87.211% \t Sparsity = 8.430% \t Test Acc = 75.957%\n",
            "Epoch 33/50 \t Loss = 0.302 \t Train Acc = 87.529% \t Sparsity = 8.225% \t Test Acc = 76.015%\n",
            "Epoch 34/50 \t Loss = 0.013 \t Train Acc = 87.826% \t Sparsity = 8.557% \t Test Acc = 76.122%\n",
            "Epoch 35/50 \t Loss = 0.100 \t Train Acc = 88.117% \t Sparsity = 8.497% \t Test Acc = 76.203%\n",
            "Epoch 36/50 \t Loss = 0.092 \t Train Acc = 88.391% \t Sparsity = 8.634% \t Test Acc = 76.318%\n",
            "Epoch 37/50 \t Loss = 0.086 \t Train Acc = 88.656% \t Sparsity = 8.640% \t Test Acc = 76.398%\n",
            "Epoch 38/50 \t Loss = 0.021 \t Train Acc = 88.902% \t Sparsity = 8.782% \t Test Acc = 76.503%\n",
            "Epoch 39/50 \t Loss = 0.039 \t Train Acc = 89.142% \t Sparsity = 8.806% \t Test Acc = 76.596%\n",
            "Epoch 40/50 \t Loss = 0.007 \t Train Acc = 89.370% \t Sparsity = 8.903% \t Test Acc = 76.696%\n",
            "Epoch 41/50 \t Loss = 0.049 \t Train Acc = 89.591% \t Sparsity = 8.903% \t Test Acc = 76.786%\n",
            "Epoch 42/50 \t Loss = 0.001 \t Train Acc = 89.802% \t Sparsity = 9.011% \t Test Acc = 76.881%\n",
            "Epoch 43/50 \t Loss = 0.033 \t Train Acc = 90.005% \t Sparsity = 9.048% \t Test Acc = 76.962%\n",
            "Epoch 44/50 \t Loss = 0.099 \t Train Acc = 90.201% \t Sparsity = 8.995% \t Test Acc = 77.039%\n",
            "Epoch 45/50 \t Loss = 0.041 \t Train Acc = 90.387% \t Sparsity = 9.093% \t Test Acc = 77.123%\n",
            "Epoch 46/50 \t Loss = 0.018 \t Train Acc = 90.568% \t Sparsity = 9.131% \t Test Acc = 77.199%\n",
            "Epoch 47/50 \t Loss = 0.021 \t Train Acc = 90.741% \t Sparsity = 9.150% \t Test Acc = 77.276%\n",
            "Epoch 48/50 \t Loss = 0.013 \t Train Acc = 90.910% \t Sparsity = 9.247% \t Test Acc = 77.349%\n",
            "Epoch 49/50 \t Loss = 0.005 \t Train Acc = 91.070% \t Sparsity = 9.255% \t Test Acc = 77.421%\n",
            "Epoch 50/50 \t Loss = 0.196 \t Train Acc = 91.226% \t Sparsity = 9.036% \t Test Acc = 77.431%\n"
          ]
        }
      ],
      "source": [
        "epochs = 50\n",
        "alpha = 0.01\n",
        "n_bits = 32\n",
        "learning_rate = 0.01\n",
        "np.set_printoptions(threshold=np.inf)\n",
        "\n",
        "model_sparsity = np.array([])\n",
        "model_train_loss = np.array([])\n",
        "model_train_acc = np.array([])\n",
        "model_test_loss = np.array([])\n",
        "model_test_acc = np.array([])\n",
        "sparsity = 0\n",
        "\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    # Iterate over the batches of the dataset.\n",
        "    loss_batch = np.array([])\n",
        "    loss_test_batch = np.array([])\n",
        "    for step, (x_batch_train, y_batch_train) in enumerate(train_ds):\n",
        "      n_zeros = 0\n",
        "      size = 0\n",
        "      #pruning\n",
        "      if alpha > 0:\n",
        "        bk = []\n",
        "        for layer_weights in model.trainable_variables:\n",
        "          if 'bn' in layer_weights.name:\n",
        "              bk.append(-1)\n",
        "          else:\n",
        "              #flatten weights\n",
        "              f_weights = tf.reshape(layer_weights,[-1])\n",
        "              #get standard deviation of each layer\n",
        "              lim = alpha*tf.math.reduce_std(f_weights)\n",
        "              bk.append(lim)\n",
        "              #create a mask\n",
        "              mask = tf.cast(tf.where(tf.abs(layer_weights)>lim,1,0), tf.float32)\n",
        "              #assign pruned weights to the layer\n",
        "              layer_weights.assign(tf.math.multiply(layer_weights,mask))\n",
        "              #check sparsity\n",
        "              flat_array = np.array((tf.reshape(mask,[-1])))\n",
        "              n_zeros += np.count_nonzero(np.array(flat_array) == 0)\n",
        "              size += flat_array.shape[0]\n",
        "              sparsity = n_zeros*100/size\n",
        "      else:\n",
        "        bk = [0] * len(model.trainable_weights)\n",
        "\n",
        "      #Cópia do modelo\n",
        "      if n_bits > 0 and alpha > 0:\n",
        "          model_copy = keras.models.clone_model(model)\n",
        "          model_copy.set_weights(model.get_weights())\n",
        "      \n",
        "      #Quantização\n",
        "      if n_bits > 0 and alpha > 0:\n",
        "          for i, layer_weights in enumerate(model.trainable_variables):\n",
        "              if 'bn' in layer_weights.name:\n",
        "                  pass\n",
        "              else:\n",
        "                  qk_line = (tf.reduce_max(tf.math.abs(layer_weights)) - bk[i]) / (2 ** (n_bits - 1) - 1)\n",
        "                  ck = tf.math.round(layer_weights / qk_line) * qk_line\n",
        "                  layer_weights.assign(ck)\n",
        "\n",
        "      with tf.GradientTape() as tape:\n",
        "        pred = model(x_batch_train, training=True)\n",
        "        loss = loss_fn(y_batch_train, pred)\n",
        "        \n",
        "      grads = tape.gradient(loss, model.trainable_weights)\n",
        "\n",
        "      if n_bits > 0 and alpha > 0:\n",
        "          for i, (layer_weights, copied_weights) in enumerate(zip(model.trainable_variables, model_copy.trainable_variables)):\n",
        "              grads[i] = grads[i] * learning_rate\n",
        "              # WEIGHT UPDATE\n",
        "              layer_weights.assign(tf.math.subtract(copied_weights, grads[i]))\n",
        "      else:\n",
        "          for i, layer_weights in enumerate(model.trainable_variables):\n",
        "              grads[i] = grads[i] * learning_rate\n",
        "              #WEIGHT UPDATE\n",
        "              layer_weights.assign(tf.math.subtract(layer_weights, grads[i]))\n",
        "          #optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
        "      \n",
        "      #optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
        "\n",
        "      predictions = tf.argmax(pred, axis=1, output_type=tf.int32)\n",
        "      acc = train_accuracy(y_batch_train, predictions)\n",
        "\n",
        "      #keep track of loss by batch\n",
        "      loss_batch = np.append(loss_batch, loss)\n",
        "\n",
        "    #mean of loss by epoch\n",
        "    model_train_loss = np.append(model_train_loss, np.mean(loss_batch))\n",
        "    #train accuracy by epoch\n",
        "    model_train_acc = np.append(model_train_acc, acc*100)\n",
        "    #sparsity by epoch\n",
        "    model_sparsity = np.append(model_sparsity, sparsity)\n",
        "\n",
        "    if alpha > 0:\n",
        "        bk = []\n",
        "        for layer_weights in model.trainable_variables:\n",
        "            if 'bn' in layer_weights.name:\n",
        "                bk.append(-1)\n",
        "            else:\n",
        "                #flatten weights\n",
        "                f_weights = tf.reshape(layer_weights,[-1])\n",
        "                #get standard deviation of each layer\n",
        "                lim = alpha*tf.math.reduce_std(f_weights)\n",
        "                bk.append(lim)\n",
        "                #create a mask\n",
        "                mask = tf.cast(tf.where(tf.abs(layer_weights)>lim,1,0), tf.float32)\n",
        "                #assign pruned weights to the layer\n",
        "                layer_weights.assign(tf.math.multiply(layer_weights,mask))\n",
        "                #check sparsity\n",
        "                flat_array = np.array((tf.reshape(mask,[-1])))\n",
        "                n_zeros += np.count_nonzero(np.array(flat_array) == 0)\n",
        "                size += flat_array.shape[0]\n",
        "                sparsity = n_zeros*100/size\n",
        "    else:\n",
        "        bk = [0] * len(model.trainable_weights)\n",
        "\n",
        "    #Quantização\n",
        "    if n_bits > 0 and alpha > 0:\n",
        "        for i, layer_weights in enumerate(model.trainable_variables):\n",
        "            if 'bn' in layer_weights.name:\n",
        "                pass\n",
        "            else:\n",
        "                qk_line = (tf.reduce_max(tf.math.abs(layer_weights)) - bk[i]) / (2 ** (n_bits - 1) - 1)\n",
        "                ck = tf.math.round(layer_weights / qk_line) * qk_line\n",
        "                layer_weights.assign(ck)\n",
        "    \n",
        "    bk.clear()\n",
        "    \n",
        "    #Test\n",
        "    for step, (x_batch_test, y_batch_test) in enumerate(test_ds):\n",
        "      test_pred = model(x_batch_test, training=False)\n",
        "      test_loss = loss_fn(y_batch_test,test_pred)\n",
        "      test_prediction = tf.argmax(test_pred, axis=1, output_type=tf.int32)\n",
        "      test_acc = test_accuracy(y_batch_test, test_prediction)\n",
        "      \n",
        "      loss_test_batch = np.append(loss_batch,test_loss)\n",
        "\n",
        "    model_test_acc = np.append(model_test_acc, test_acc*100)\n",
        "    model_test_loss = np.append(model_test_loss,np.mean(loss_test_batch))\n",
        "    \n",
        "    print(\"Epoch {}/{} \\t Loss = {:.3f} \\t Train Acc = {:.3f}% \\t Sparsity = {:.3f}% \\t Test Acc = {:.3f}%\".format(epoch+1,epochs,float(loss),float(acc*100),sparsity,float(test_acc*100)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qH9TwrMnc98a"
      },
      "outputs": [],
      "source": [
        "l = len(model.trainable_weights)\n",
        "for i in range(l):\n",
        "  a = tf.reshape(model.trainable_weights[i],[-1])\n",
        "  b = a.numpy()\n",
        "  #print(a)\n",
        "  #plt.ylim(0,300)\n",
        "  plt.title(str(i))\n",
        "  plt.hist(b,200)\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5eo_wg9tc98b"
      },
      "outputs": [],
      "source": [
        "plt.title(\"test acc x train acc\")\n",
        "plt.plot(model_train_acc)\n",
        "plt.plot(model_test_acc)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RaQc5JNFc98c"
      },
      "outputs": [],
      "source": [
        "plt.title(\"train loss x test loss\")\n",
        "plt.plot(model_train_loss)\n",
        "plt.plot(model_test_loss)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ioeGU-jjc98c"
      },
      "outputs": [],
      "source": [
        "plt.title(\"Sparsity\")\n",
        "plt.plot(model_sparsity)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oSh5m9tTc98d"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import itertools\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_confusion_matrix(cm, classes,\n",
        "                        normalize=False,\n",
        "                        title='Confusion matrix',\n",
        "                        cmap=plt.cm.Blues):\n",
        "    \"\"\"\n",
        "    This function prints and plots the confusion matrix.\n",
        "    Normalization can be applied by setting `normalize=True`.\n",
        "    \"\"\"\n",
        "    figure(figsize=(10, 7), dpi=80)\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(title)\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(10)\n",
        "    plt.xticks(tick_marks, classes, rotation=45)\n",
        "    plt.yticks(tick_marks, classes)\n",
        "\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "        print(\"Normalized confusion matrix\")\n",
        "    else:\n",
        "        print('Confusion matrix, without normalization')\n",
        "\n",
        "    #print(cm[:,0])\n",
        "\n",
        "    thresh = cm.max() / 2.\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        plt.text(j, i, str(cm[i, j]*100/1000) + \"%\",\n",
        "            horizontalalignment=\"center\",\n",
        "            color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.ylabel('Classe real')\n",
        "    plt.xlabel('Classe predita')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qwd39wm-c98e"
      },
      "outputs": [],
      "source": [
        "test_accuracy = tf.keras.metrics.Accuracy()\n",
        "logits = model(x_test, training=False)\n",
        "prediction = tf.argmax(logits, axis=1, output_type=tf.int32)\n",
        "test_accuracy(prediction, y_test)\n",
        "print(\"Test set accuracy: {:.3%}\".format(test_accuracy.result()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BeB7WM_Xc98f"
      },
      "outputs": [],
      "source": [
        "classes = [\"avião\",\"carro\",\"pássaro\",\"gato\",\"veado\",\"cachorro\",\"sapo\",\"cavalo\",\"navio\",\"caminhão\"]\n",
        "cm = confusion_matrix(y_true=y_test, y_pred=prediction)\n",
        "plot_confusion_matrix(cm=cm, classes=classes, title='Matriz de confusão')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "SkAsvvxbhb9S"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "dl_newModel.ipynb",
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}