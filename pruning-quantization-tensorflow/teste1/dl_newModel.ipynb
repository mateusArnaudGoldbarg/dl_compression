{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Q8lqzbTc98R",
        "outputId": "387cebbd-5a80-464f-ebf4-973640f8d559"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2.7.0\n",
            "0\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.pyplot import figure\n",
        "\n",
        "print(tf.__version__)\n",
        "print(len(tf.config.experimental.list_physical_devices('GPU')))\n",
        "device_name = tf.test.gpu_device_name()\n",
        "print(device_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v_J7hEAkc98W",
        "outputId": "98ed88aa-bf6f-4a26-8d79-adb6a1e57126"
      },
      "outputs": [],
      "source": [
        "#IMPORTAÇÃO E NORRMALIZAÇÃO\n",
        "(x_train, y_train), (x_test,y_test) = keras.datasets.cifar10.load_data()\n",
        "#x_train = x_train.reshape(-1, 28, 28, 1)\n",
        "#x_test = x_test.reshape(-1, 28, 28, 1)\n",
        "\n",
        "x_train = x_train.astype(float)/255\n",
        "x_test = x_test.astype(float)/255\n",
        "\n",
        "#CRIAR DATASET\n",
        "train_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(50000).batch(64)\n",
        "test_ds = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(64)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "7JY0tvhQc98X"
      },
      "outputs": [],
      "source": [
        "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False)\n",
        "optimizer = tf.keras.optimizers.SGD(learning_rate=0.1)\n",
        "acc_metric = keras.metrics.SparseCategoricalAccuracy()\n",
        "test_accuracy = tf.keras.metrics.Accuracy()\n",
        "test_loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "train_accuracy = tf.keras.metrics.Accuracy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lxIyhmBac98Y",
        "outputId": "e103f353-41b1-4ebc-b0ba-ca88ea457c67"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"modelo-0.0\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv1 (Conv2D)              (64, 32, 32, 32)          896       \n",
            "                                                                 \n",
            " bn1 (BatchNormalization)    (64, 32, 32, 32)          128       \n",
            "                                                                 \n",
            " conv2 (Conv2D)              (64, 32, 32, 32)          9248      \n",
            "                                                                 \n",
            " bn2 (BatchNormalization)    (64, 32, 32, 32)          128       \n",
            "                                                                 \n",
            " mp1 (MaxPooling2D)          (64, 16, 16, 32)          0         \n",
            "                                                                 \n",
            " conv3 (Conv2D)              (64, 16, 16, 64)          18496     \n",
            "                                                                 \n",
            " bn3 (BatchNormalization)    (64, 16, 16, 64)          256       \n",
            "                                                                 \n",
            " conv4 (Conv2D)              (64, 16, 16, 64)          36928     \n",
            "                                                                 \n",
            " bn4 (BatchNormalization)    (64, 16, 16, 64)          256       \n",
            "                                                                 \n",
            " mp2 (MaxPooling2D)          (64, 8, 8, 64)            0         \n",
            "                                                                 \n",
            " conv5 (Conv2D)              (64, 8, 8, 128)           73856     \n",
            "                                                                 \n",
            " bn5 (BatchNormalization)    (64, 8, 8, 128)           512       \n",
            "                                                                 \n",
            " conv6 (Conv2D)              (64, 8, 8, 128)           147584    \n",
            "                                                                 \n",
            " bn6 (BatchNormalization)    (64, 8, 8, 128)           512       \n",
            "                                                                 \n",
            " mp3 (MaxPooling2D)          (64, 4, 4, 128)           0         \n",
            "                                                                 \n",
            " flatten (Flatten)           (64, 2048)                0         \n",
            "                                                                 \n",
            " dp1 (Dropout)               (64, 2048)                0         \n",
            "                                                                 \n",
            " dense1 (Dense)              (64, 1024)                2098176   \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (64, 1024)                0         \n",
            "                                                                 \n",
            " dense4 (Dense)              (64, 10)                  10250     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2,397,226\n",
            "Trainable params: 2,396,330\n",
            "Non-trainable params: 896\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model = keras.Sequential(\n",
        "    [\n",
        "        keras.layers.InputLayer(input_shape=(32, 32,3),batch_size=64),\n",
        "     \n",
        "        keras.layers.Conv2D(32, (3, 3), name=\"conv1\", activation='relu',padding='same'),\n",
        "        keras.layers.BatchNormalization(name=\"bn1\"),  \n",
        "        keras.layers.Conv2D(32, (3, 3),name=\"conv2\", activation='relu', padding='same'),\n",
        "        keras.layers.BatchNormalization(name=\"bn2\"),\n",
        "        keras.layers.MaxPooling2D(pool_size=(2, 2),name=\"mp1\"),\n",
        "        #keras.layers.Dropout(0.2),\n",
        "        keras.layers.Conv2D(64, (3, 3),name=\"conv3\", activation='relu', padding='same'),\n",
        "        keras.layers.BatchNormalization(name=\"bn3\"),\n",
        "        keras.layers.Conv2D(64, (3, 3),name=\"conv4\", activation='relu', padding='same'),\n",
        "        keras.layers.BatchNormalization(name=\"bn4\"),\n",
        "        keras.layers.MaxPooling2D(pool_size=(2, 2),name=\"mp2\"),\n",
        "        #keras.layers.Dropout(0.2),\n",
        "        keras.layers.Conv2D(128, (3, 3),name=\"conv5\", activation='relu', padding='same'),\n",
        "        keras.layers.BatchNormalization(name=\"bn5\"),\n",
        "        keras.layers.Conv2D(128, (3, 3),name=\"conv6\", activation='relu', padding='same'),\n",
        "        keras.layers.BatchNormalization(name=\"bn6\"),\n",
        "        keras.layers.MaxPooling2D(pool_size=(2, 2),name=\"mp3\"),\n",
        "        #keras.layers.Dropout(0.2),\n",
        "     \n",
        "        keras.layers.Flatten(name = \"flatten\"),\n",
        "        keras.layers.Dropout(0.2, name=\"dp1\"),\n",
        "        keras.layers.Dense(1024, name=\"dense1\", activation='relu'),\n",
        "        keras.layers.Dropout(0.2),\n",
        "        #keras.layers.Dense(256, name=\"dense2\", activation='relu'),\n",
        "        #keras.layers.Dropout(0.4),\n",
        "        #keras.layers.Dense(128, name=\"dense3\", activation='relu'),\n",
        "        #keras.layers.BatchNormalization(name=\"bn7\"),\n",
        "        #keras.layers.Dropout(0.4),\n",
        "        keras.layers.Dense(10, name=\"dense4\", activation='softmax')\n",
        "    ],\n",
        "    name=\"modelo-0.0\",\n",
        ")\n",
        "\n",
        "for layer in model.trainable_variables:\n",
        "    #print(layer.name)\n",
        "    if 'bias' in layer.name:\n",
        "        new_bias = tf.cast(tf.where(tf.abs(layer) >= 0, 0.1, 0.1), tf.float32)\n",
        "        layer.assign(new_bias)\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lNxvzk3pc98Z",
        "outputId": "ad823622-926d-4976-b25d-31e6484382d7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<__array_function__ internals>:5: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/300 \t Loss = 1.147 \t Train Acc = 46.220% \t Sparsity = 47.545% \t Test Acc = 46.380%\n",
            "New test is 46.380%, Model saved\n",
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
            "Epoch 2/300 \t Loss = 1.128 \t Train Acc = 54.883% \t Sparsity = 51.866% \t Test Acc = 55.865%\n",
            "New test is 55.865%, Model saved\n",
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
            "Epoch 3/300 \t Loss = 0.918 \t Train Acc = 60.570% \t Sparsity = 54.937% \t Test Acc = 61.113%\n",
            "New test is 61.113%, Model saved\n",
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
            "Epoch 4/300 \t Loss = 1.036 \t Train Acc = 64.536% \t Sparsity = 57.381% \t Test Acc = 64.268%\n",
            "New test is 64.268%, Model saved\n",
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
            "Epoch 5/300 \t Loss = 0.408 \t Train Acc = 67.498% \t Sparsity = 59.724% \t Test Acc = 66.816%\n",
            "New test is 66.816%, Model saved\n",
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
            "Epoch 6/300 \t Loss = 0.476 \t Train Acc = 69.936% \t Sparsity = 61.884% \t Test Acc = 68.505%\n",
            "New test is 68.505%, Model saved\n",
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
            "Epoch 7/300 \t Loss = 0.716 \t Train Acc = 71.971% \t Sparsity = 63.876% \t Test Acc = 69.874%\n",
            "New test is 69.874%, Model saved\n",
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
            "Epoch 8/300 \t Loss = 0.192 \t Train Acc = 73.737% \t Sparsity = 65.704% \t Test Acc = 70.957%\n",
            "New test is 70.957%, Model saved\n",
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
            "Epoch 9/300 \t Loss = 0.333 \t Train Acc = 75.282% \t Sparsity = 67.359% \t Test Acc = 71.773%\n",
            "New test is 71.773%, Model saved\n",
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
            "Epoch 10/300 \t Loss = 0.185 \t Train Acc = 76.609% \t Sparsity = 68.867% \t Test Acc = 72.519%\n",
            "New test is 72.519%, Model saved\n",
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        }
      ],
      "source": [
        "epochs = 300\n",
        "alpha = 0.5\n",
        "n_bits = 32\n",
        "\n",
        "learning_rate = 0.03\n",
        "momentum = 0.9\n",
        "\n",
        "np.set_printoptions(threshold=np.inf)\n",
        "\n",
        "model_sparsity = np.array([])\n",
        "model_train_loss = np.array([])\n",
        "model_train_acc = np.array([])\n",
        "model_test_loss = np.array([])\n",
        "model_test_acc = np.array([])\n",
        "sparsity = 0\n",
        "\n",
        "#metrics to save model\n",
        "last_test_acc = 0\n",
        "model_name = \"alpha_05_bits_32_lr_003.h5\"\n",
        "\n",
        "#TODO: Increase batch_size and just quantize batch_norm layer\n",
        "with tf.device('/device:GPU:0'):\n",
        "    for epoch in range(epochs):\n",
        "        # Iterate over the batches of the dataset.\n",
        "        loss_batch = np.array([])\n",
        "        loss_test_batch = np.array([])\n",
        "        for step, (x_batch_train, y_batch_train) in enumerate(train_ds):\n",
        "          n_zeros = 0\n",
        "          size = 0\n",
        "          #pruning\n",
        "          if alpha > 0:\n",
        "            bk = []\n",
        "            for layer_weights in model.trainable_variables:\n",
        "              if 'bn' in layer_weights.name:\n",
        "                  bk.append(-1)\n",
        "              else:\n",
        "                  #flatten weights\n",
        "                  f_weights = tf.reshape(layer_weights,[-1])\n",
        "                  #get standard deviation of each layer\n",
        "                  lim = alpha*tf.math.reduce_std(f_weights)\n",
        "                  bk.append(lim)\n",
        "                  #create a mask\n",
        "                  mask = tf.cast(tf.where(tf.abs(layer_weights)>lim,1,0), tf.float32)\n",
        "                  #assign pruned weights to the layer\n",
        "                  layer_weights.assign(tf.math.multiply(layer_weights,mask))\n",
        "                  #check sparsity\n",
        "                  flat_array = np.array((tf.reshape(mask,[-1])))\n",
        "                  n_zeros += np.count_nonzero(np.array(flat_array) == 0)\n",
        "                  size += flat_array.shape[0]\n",
        "                  sparsity = n_zeros*100/size\n",
        "          else:\n",
        "            bk = [0] * len(model.trainable_weights)\n",
        "\n",
        "          #Cópia do modelo\n",
        "          if n_bits > 0 and alpha > 0:\n",
        "              model_copy = keras.models.clone_model(model)\n",
        "              model_copy.set_weights(model.get_weights())\n",
        "          \n",
        "          #Quantização\n",
        "          if n_bits > 0 and alpha > 0:\n",
        "              for i, layer_weights in enumerate(model.trainable_variables):\n",
        "                  if 'bn' in layer_weights.name:\n",
        "                      pass\n",
        "                  else:\n",
        "                      qk_line = (tf.reduce_max(tf.math.abs(layer_weights)) - bk[i]) / (2 ** (n_bits - 1) - 1)\n",
        "                      ck = tf.math.round(layer_weights / qk_line) * qk_line\n",
        "                      layer_weights.assign(ck)\n",
        "\n",
        "          with tf.GradientTape() as tape:\n",
        "            pred = model(x_batch_train, training=True)\n",
        "            loss = loss_fn(y_batch_train, pred)\n",
        "            \n",
        "          grads = tape.gradient(loss, model.trainable_weights)\n",
        "          if step==0 and epoch==0:\n",
        "              v = np.zeros_like(grads)\n",
        "          if n_bits > 0 and alpha > 0:\n",
        "              for i, (layer_weights, copied_weights) in enumerate(zip(model.trainable_variables, model_copy.trainable_variables)):\n",
        "                  #TODO: Add momentum and velocity\n",
        "                  grads[i] = grads[i] * learning_rate\n",
        "                  #Get value of velocity\n",
        "                  v[i] = tf.math.subtract(momentum*v[i], grads[i])\n",
        "                  # WEIGHT UPDATE\n",
        "                  layer_weights.assign(tf.math.add(copied_weights, v[i]))\n",
        "          else:\n",
        "              for i, layer_weights in enumerate(model.trainable_variables):\n",
        "                  #TODO: Include momentum here\n",
        "                  grads[i] = grads[i] * learning_rate\n",
        "                  #WEIGHT UPDATE\n",
        "                  layer_weights.assign(tf.math.subtract(layer_weights, grads[i]))\n",
        "              #optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
        "          \n",
        "          #optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
        "\n",
        "          predictions = tf.argmax(pred, axis=1, output_type=tf.int32)\n",
        "          acc = train_accuracy(y_batch_train, predictions)\n",
        "\n",
        "          #keep track of loss by batch\n",
        "          loss_batch = np.append(loss_batch, loss)\n",
        "\n",
        "        #mean of loss by epoch\n",
        "        model_train_loss = np.append(model_train_loss, np.mean(loss_batch))\n",
        "        #train accuracy by epoch\n",
        "        model_train_acc = np.append(model_train_acc, acc*100)\n",
        "        #sparsity by epoch\n",
        "        model_sparsity = np.append(model_sparsity, sparsity)\n",
        "\n",
        "        if alpha > 0:\n",
        "            bk = []\n",
        "            for layer_weights in model.trainable_variables:\n",
        "                if 'bn' in layer_weights.name:\n",
        "                    bk.append(-1)\n",
        "                else:\n",
        "                    #flatten weights\n",
        "                    f_weights = tf.reshape(layer_weights,[-1])\n",
        "                    #get standard deviation of each layer\n",
        "                    lim = alpha*tf.math.reduce_std(f_weights)\n",
        "                    bk.append(lim)\n",
        "                    #create a mask\n",
        "                    mask = tf.cast(tf.where(tf.abs(layer_weights)>lim,1,0), tf.float32)\n",
        "                    #assign pruned weights to the layer\n",
        "                    layer_weights.assign(tf.math.multiply(layer_weights,mask))\n",
        "                    #check sparsity\n",
        "                    flat_array = np.array((tf.reshape(mask,[-1])))\n",
        "                    n_zeros += np.count_nonzero(np.array(flat_array) == 0)\n",
        "                    size += flat_array.shape[0]\n",
        "                    sparsity = n_zeros*100/size\n",
        "        else:\n",
        "            bk = [0] * len(model.trainable_weights)\n",
        "\n",
        "        #Quantização\n",
        "        if n_bits > 0 and alpha > 0:\n",
        "            for i, layer_weights in enumerate(model.trainable_variables):\n",
        "                if 'bn' in layer_weights.name:\n",
        "                    pass\n",
        "                else:\n",
        "                    qk_line = (tf.reduce_max(tf.math.abs(layer_weights)) - bk[i]) / (2 ** (n_bits - 1) - 1)\n",
        "                    ck = tf.math.round(layer_weights / qk_line) * qk_line\n",
        "                    layer_weights.assign(ck)\n",
        "        \n",
        "        bk.clear()\n",
        "        \n",
        "        #Test\n",
        "        for step, (x_batch_test, y_batch_test) in enumerate(test_ds):\n",
        "          test_pred = model(x_batch_test, training=False)\n",
        "          test_loss = loss_fn(y_batch_test,test_pred)\n",
        "          test_prediction = tf.argmax(test_pred, axis=1, output_type=tf.int32)\n",
        "          test_acc = test_accuracy(y_batch_test, test_prediction)\n",
        "          #TODO: test accuracy by mean of the batch_valudation\n",
        "          loss_test_batch = np.append(loss_batch,test_loss)\n",
        "\n",
        "        model_test_acc = np.append(model_test_acc, test_acc*100)\n",
        "        model_test_loss = np.append(model_test_loss,np.mean(loss_test_batch))\n",
        "        \n",
        "        print(\"Epoch {}/{} \\t Loss = {:.3f} \\t Train Acc = {:.3f}% \\t Sparsity = {:.3f}% \\t Test Acc = {:.3f}%\".format(epoch+1,epochs,float(loss),float(acc*100),sparsity,float(test_acc*100)))\n",
        "\n",
        "        #Save model with best test accuracy\n",
        "        if test_acc > last_test_acc:\n",
        "            print('New test is {:.3f}%, Model saved'.format(test_acc*100))\n",
        "            last_test_acc = test_acc\n",
        "            model.save(model_name)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qH9TwrMnc98a"
      },
      "outputs": [],
      "source": [
        "l = len(model.trainable_weights)\n",
        "for i in range(l):\n",
        "  a = tf.reshape(model.trainable_weights[i],[-1])\n",
        "  b = a.numpy()\n",
        "  #print(a)\n",
        "  #plt.ylim(0,300)\n",
        "  plt.title(str(i))\n",
        "  plt.hist(b,200)\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5eo_wg9tc98b"
      },
      "outputs": [],
      "source": [
        "plt.title(\"test acc x train acc\")\n",
        "plt.plot(model_train_acc)\n",
        "plt.plot(model_test_acc)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RaQc5JNFc98c"
      },
      "outputs": [],
      "source": [
        "plt.title(\"train loss x test loss\")\n",
        "plt.plot(model_train_loss)\n",
        "plt.plot(model_test_loss)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ioeGU-jjc98c"
      },
      "outputs": [],
      "source": [
        "plt.title(\"Sparsity\")\n",
        "plt.plot(model_sparsity)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oSh5m9tTc98d"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import itertools\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_confusion_matrix(cm, classes,\n",
        "                        normalize=False,\n",
        "                        title='Confusion matrix',\n",
        "                        cmap=plt.cm.Blues):\n",
        "    \"\"\"\n",
        "    This function prints and plots the confusion matrix.\n",
        "    Normalization can be applied by setting `normalize=True`.\n",
        "    \"\"\"\n",
        "    figure(figsize=(10, 7), dpi=80)\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(title)\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(10)\n",
        "    plt.xticks(tick_marks, classes, rotation=45)\n",
        "    plt.yticks(tick_marks, classes)\n",
        "\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "        print(\"Normalized confusion matrix\")\n",
        "    else:\n",
        "        print('Confusion matrix, without normalization')\n",
        "\n",
        "    #print(cm[:,0])\n",
        "\n",
        "    thresh = cm.max() / 2.\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        plt.text(j, i, str(cm[i, j]*100/1000) + \"%\",\n",
        "            horizontalalignment=\"center\",\n",
        "            color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.ylabel('Classe real')\n",
        "    plt.xlabel('Classe predita')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qwd39wm-c98e"
      },
      "outputs": [],
      "source": [
        "test_accuracy = tf.keras.metrics.Accuracy()\n",
        "logits = model(x_test, training=False)\n",
        "prediction = tf.argmax(logits, axis=1, output_type=tf.int32)\n",
        "test_accuracy(prediction, y_test)\n",
        "print(\"Test set accuracy: {:.3%}\".format(test_accuracy.result()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BeB7WM_Xc98f"
      },
      "outputs": [],
      "source": [
        "classes = [\"avião\",\"carro\",\"pássaro\",\"gato\",\"veado\",\"cachorro\",\"sapo\",\"cavalo\",\"navio\",\"caminhão\"]\n",
        "cm = confusion_matrix(y_true=y_test, y_pred=prediction)\n",
        "plot_confusion_matrix(cm=cm, classes=classes, title='Matriz de confusão')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SkAsvvxbhb9S"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "dl_newModel.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
