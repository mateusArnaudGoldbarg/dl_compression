{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.7.0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "\n",
    "print(tf.__version__)\n",
    "print(len(tf.config.experimental.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#IMPORTAÇÃO E NORRMALIZAÇÃO\n",
    "(x_train, y_train), (x_test,y_test) = keras.datasets.cifar10.load_data()\n",
    "#x_train = x_train.reshape(-1, 28, 28, 1)\n",
    "#x_test = x_test.reshape(-1, 28, 28, 1)\n",
    "\n",
    "x_train = x_train.astype(float)/255\n",
    "x_test = x_test.astype(float)/255\n",
    "\n",
    "#CRIAR DATASET\n",
    "train_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(50000).batch(64)\n",
    "test_ds = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False)\n",
    "optimizer = tf.keras.optimizers.SGD()\n",
    "acc_metric = keras.metrics.SparseCategoricalAccuracy()\n",
    "test_accuracy = tf.keras.metrics.Accuracy()\n",
    "test_loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "train_accuracy = tf.keras.metrics.Accuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1/kernel:0\n",
      "conv1/bias:0\n",
      "bn1/gamma:0\n",
      "bn1/beta:0\n",
      "conv2/kernel:0\n",
      "conv2/bias:0\n",
      "bn2/gamma:0\n",
      "bn2/beta:0\n",
      "conv3/kernel:0\n",
      "conv3/bias:0\n",
      "bn3/gamma:0\n",
      "bn3/beta:0\n",
      "conv4/kernel:0\n",
      "conv4/bias:0\n",
      "bn4/gamma:0\n",
      "bn4/beta:0\n",
      "conv5/kernel:0\n",
      "conv5/bias:0\n",
      "bn5/gamma:0\n",
      "bn5/beta:0\n",
      "conv6/kernel:0\n",
      "conv6/bias:0\n",
      "bn6/gamma:0\n",
      "bn6/beta:0\n",
      "dense1/kernel:0\n",
      "dense1/bias:0\n",
      "dense2/kernel:0\n",
      "dense2/bias:0\n",
      "bn7/gamma:0\n",
      "bn7/beta:0\n",
      "dense3/kernel:0\n",
      "dense3/bias:0\n",
      "Model: \"modelo-0.0\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv1 (Conv2D)              (64, 32, 32, 32)          896       \n",
      "                                                                 \n",
      " bn1 (BatchNormalization)    (64, 32, 32, 32)          128       \n",
      "                                                                 \n",
      " conv2 (Conv2D)              (64, 32, 32, 32)          9248      \n",
      "                                                                 \n",
      " bn2 (BatchNormalization)    (64, 32, 32, 32)          128       \n",
      "                                                                 \n",
      " mp1 (MaxPooling2D)          (64, 16, 16, 32)          0         \n",
      "                                                                 \n",
      " dropout (Dropout)           (64, 16, 16, 32)          0         \n",
      "                                                                 \n",
      " conv3 (Conv2D)              (64, 16, 16, 64)          18496     \n",
      "                                                                 \n",
      " bn3 (BatchNormalization)    (64, 16, 16, 64)          256       \n",
      "                                                                 \n",
      " conv4 (Conv2D)              (64, 16, 16, 64)          36928     \n",
      "                                                                 \n",
      " bn4 (BatchNormalization)    (64, 16, 16, 64)          256       \n",
      "                                                                 \n",
      " mp2 (MaxPooling2D)          (64, 8, 8, 64)            0         \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (64, 8, 8, 64)            0         \n",
      "                                                                 \n",
      " conv5 (Conv2D)              (64, 8, 8, 128)           73856     \n",
      "                                                                 \n",
      " bn5 (BatchNormalization)    (64, 8, 8, 128)           512       \n",
      "                                                                 \n",
      " conv6 (Conv2D)              (64, 8, 8, 128)           147584    \n",
      "                                                                 \n",
      " bn6 (BatchNormalization)    (64, 8, 8, 128)           512       \n",
      "                                                                 \n",
      " mp3 (MaxPooling2D)          (64, 4, 4, 128)           0         \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (64, 4, 4, 128)           0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (64, 2048)                0         \n",
      "                                                                 \n",
      " dense1 (Dense)              (64, 256)                 524544    \n",
      "                                                                 \n",
      " dense2 (Dense)              (64, 128)                 32896     \n",
      "                                                                 \n",
      " bn7 (BatchNormalization)    (64, 128)                 512       \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (64, 128)                 0         \n",
      "                                                                 \n",
      " dense3 (Dense)              (64, 10)                  1290      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 848,042\n",
      "Trainable params: 846,890\n",
      "Non-trainable params: 1,152\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = keras.Sequential(\n",
    "    [\n",
    "        keras.layers.InputLayer(input_shape=(32, 32,3),batch_size=64),\n",
    "     \n",
    "        keras.layers.Conv2D(32, (3, 3), name=\"conv1\", activation='relu', kernel_initializer='he_uniform', padding='same'),\n",
    "        keras.layers.BatchNormalization(name=\"bn1\"),  \n",
    "        keras.layers.Conv2D(32, (3, 3),name=\"conv2\", activation='relu', kernel_initializer='he_uniform', padding='same'),\n",
    "        keras.layers.BatchNormalization(name=\"bn2\"),\n",
    "        keras.layers.MaxPooling2D(pool_size=(2, 2),name=\"mp1\"),\n",
    "        keras.layers.Dropout(0.2),\n",
    "        keras.layers.Conv2D(64, (3, 3),name=\"conv3\", activation='relu', kernel_initializer='he_uniform', padding='same'),\n",
    "        keras.layers.BatchNormalization(name=\"bn3\"),\n",
    "        keras.layers.Conv2D(64, (3, 3),name=\"conv4\", activation='relu', kernel_initializer='he_uniform', padding='same'),\n",
    "        keras.layers.BatchNormalization(name=\"bn4\"),\n",
    "        keras.layers.MaxPooling2D(pool_size=(2, 2),name=\"mp2\"),\n",
    "        keras.layers.Dropout(0.2),\n",
    "        keras.layers.Conv2D(128, (3, 3),name=\"conv5\", activation='relu', kernel_initializer='he_uniform', padding='same'),\n",
    "        keras.layers.BatchNormalization(name=\"bn5\"),\n",
    "        keras.layers.Conv2D(128, (3, 3),name=\"conv6\", activation='relu', kernel_initializer='he_uniform', padding='same'),\n",
    "        keras.layers.BatchNormalization(name=\"bn6\"),\n",
    "        keras.layers.MaxPooling2D(pool_size=(2, 2),name=\"mp3\"),\n",
    "        keras.layers.Dropout(0.2),\n",
    "     \n",
    "        keras.layers.Flatten(name = \"flatten\"),\n",
    "     \n",
    "        keras.layers.Dense(256, name=\"dense1\", activation='relu'),\n",
    "        keras.layers.Dense(128, name=\"dense2\", activation='relu', kernel_initializer='he_uniform'),\n",
    "        keras.layers.BatchNormalization(name=\"bn7\"),\n",
    "        keras.layers.Dropout(0.2),\n",
    "        keras.layers.Dense(10, name=\"dense3\", activation='softmax')\n",
    "    ],\n",
    "    name=\"modelo-0.0\",\n",
    ")\n",
    "\n",
    "for layer in model.trainable_variables:\n",
    "    print(layer.name)\n",
    "    if 'bias' in layer.name:\n",
    "        new_bias = tf.cast(tf.where(tf.abs(layer) >= 0, 0.1, 0.1), tf.float32)\n",
    "        layer.assign(new_bias)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300 \t Loss = 1.978 \t Train Acc = 48.538% \t Sparsity = 38.385% \t Test Acc = 53.470%\n",
      "Epoch 2/300 \t Loss = 1.030 \t Train Acc = 55.516% \t Sparsity = 41.787% \t Test Acc = 57.995%\n",
      "Epoch 3/300 \t Loss = 1.343 \t Train Acc = 59.895% \t Sparsity = 43.832% \t Test Acc = 61.250%\n",
      "Epoch 4/300 \t Loss = 1.086 \t Train Acc = 62.908% \t Sparsity = 46.708% \t Test Acc = 63.447%\n",
      "Epoch 5/300 \t Loss = 0.569 \t Train Acc = 65.284% \t Sparsity = 48.397% \t Test Acc = 65.490%\n",
      "Epoch 6/300 \t Loss = 0.739 \t Train Acc = 67.090% \t Sparsity = 49.490% \t Test Acc = 66.932%\n",
      "Epoch 7/300 \t Loss = 0.649 \t Train Acc = 68.567% \t Sparsity = 51.203% \t Test Acc = 68.129%\n",
      "Epoch 8/300 \t Loss = 0.628 \t Train Acc = 69.818% \t Sparsity = 52.288% \t Test Acc = 68.409%\n",
      "Epoch 9/300 \t Loss = 0.478 \t Train Acc = 70.919% \t Sparsity = 53.368% \t Test Acc = 69.292%\n",
      "Epoch 10/300 \t Loss = 0.726 \t Train Acc = 71.834% \t Sparsity = 54.163% \t Test Acc = 70.161%\n",
      "Epoch 11/300 \t Loss = 0.751 \t Train Acc = 72.670% \t Sparsity = 55.123% \t Test Acc = 70.950%\n",
      "Epoch 12/300 \t Loss = 0.894 \t Train Acc = 73.404% \t Sparsity = 55.829% \t Test Acc = 71.614%\n",
      "Epoch 13/300 \t Loss = 1.098 \t Train Acc = 74.066% \t Sparsity = 56.334% \t Test Acc = 72.161%\n",
      "Epoch 14/300 \t Loss = 0.583 \t Train Acc = 74.674% \t Sparsity = 56.747% \t Test Acc = 72.616%\n",
      "Epoch 15/300 \t Loss = 0.561 \t Train Acc = 75.215% \t Sparsity = 57.049% \t Test Acc = 73.050%\n",
      "Epoch 16/300 \t Loss = 1.387 \t Train Acc = 75.699% \t Sparsity = 57.335% \t Test Acc = 73.488%\n",
      "Epoch 17/300 \t Loss = 0.726 \t Train Acc = 76.137% \t Sparsity = 57.587% \t Test Acc = 73.874%\n",
      "Epoch 18/300 \t Loss = 0.798 \t Train Acc = 76.551% \t Sparsity = 57.768% \t Test Acc = 74.228%\n",
      "Epoch 19/300 \t Loss = 0.602 \t Train Acc = 76.918% \t Sparsity = 57.918% \t Test Acc = 74.490%\n",
      "Epoch 20/300 \t Loss = 0.741 \t Train Acc = 77.259% \t Sparsity = 58.033% \t Test Acc = 74.534%\n",
      "Epoch 21/300 \t Loss = 0.527 \t Train Acc = 77.570% \t Sparsity = 58.122% \t Test Acc = 74.787%\n",
      "Epoch 22/300 \t Loss = 0.719 \t Train Acc = 77.856% \t Sparsity = 58.182% \t Test Acc = 74.915%\n",
      "Epoch 23/300 \t Loss = 0.745 \t Train Acc = 78.128% \t Sparsity = 58.232% \t Test Acc = 75.141%\n",
      "Epoch 24/300 \t Loss = 0.427 \t Train Acc = 78.376% \t Sparsity = 58.271% \t Test Acc = 75.362%\n",
      "Epoch 25/300 \t Loss = 0.451 \t Train Acc = 78.604% \t Sparsity = 58.296% \t Test Acc = 75.490%\n",
      "Epoch 26/300 \t Loss = 0.446 \t Train Acc = 78.823% \t Sparsity = 58.317% \t Test Acc = 75.654%\n",
      "Epoch 27/300 \t Loss = 0.516 \t Train Acc = 79.022% \t Sparsity = 58.334% \t Test Acc = 75.823%\n",
      "Epoch 28/300 \t Loss = 0.254 \t Train Acc = 79.213% \t Sparsity = 58.348% \t Test Acc = 75.975%\n",
      "Epoch 29/300 \t Loss = 0.172 \t Train Acc = 79.386% \t Sparsity = 58.355% \t Test Acc = 76.117%\n",
      "Epoch 30/300 \t Loss = 1.341 \t Train Acc = 79.556% \t Sparsity = 58.360% \t Test Acc = 76.246%\n",
      "Epoch 31/300 \t Loss = 1.099 \t Train Acc = 79.702% \t Sparsity = 58.365% \t Test Acc = 76.396%\n",
      "Epoch 32/300 \t Loss = 0.623 \t Train Acc = 79.845% \t Sparsity = 58.368% \t Test Acc = 76.526%\n",
      "Epoch 33/300 \t Loss = 0.832 \t Train Acc = 79.983% \t Sparsity = 58.371% \t Test Acc = 76.410%\n",
      "Epoch 34/300 \t Loss = 0.228 \t Train Acc = 80.105% \t Sparsity = 58.373% \t Test Acc = 76.536%\n",
      "Epoch 35/300 \t Loss = 0.155 \t Train Acc = 80.231% \t Sparsity = 58.375% \t Test Acc = 76.629%\n",
      "Epoch 36/300 \t Loss = 0.995 \t Train Acc = 80.345% \t Sparsity = 58.377% \t Test Acc = 76.720%\n",
      "Epoch 37/300 \t Loss = 0.770 \t Train Acc = 80.454% \t Sparsity = 58.378% \t Test Acc = 76.829%\n",
      "Epoch 38/300 \t Loss = 0.234 \t Train Acc = 80.563% \t Sparsity = 58.379% \t Test Acc = 76.926%\n",
      "Epoch 39/300 \t Loss = 0.679 \t Train Acc = 80.661% \t Sparsity = 58.379% \t Test Acc = 77.027%\n",
      "Epoch 40/300 \t Loss = 0.759 \t Train Acc = 80.749% \t Sparsity = 58.380% \t Test Acc = 77.089%\n",
      "Epoch 41/300 \t Loss = 0.146 \t Train Acc = 80.830% \t Sparsity = 58.381% \t Test Acc = 77.138%\n"
     ]
    }
   ],
   "source": [
    "epochs = 300\n",
    "alpha = 0.5\n",
    "n_bits = 4\n",
    "learning_rate = 0.075\n",
    "np.set_printoptions(threshold=np.inf)\n",
    "\n",
    "model_sparsity = np.array([])\n",
    "model_sparsity_layers = np.array([])\n",
    "model_train_loss = np.array([])\n",
    "model_train_acc = np.array([])\n",
    "model_test_loss = np.array([])\n",
    "model_test_acc = np.array([])\n",
    "sparsity = 0\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # Iterate over the batches of the dataset.\n",
    "    acc_epoch_mean = np.array([])\n",
    "    loss_epoch_mean = np.array([])\n",
    "\n",
    "    acct_epoch_mean = np.array([])\n",
    "    losst_epoch_mean = np.array([])\n",
    "    for step, (x_batch_train, y_batch_train) in enumerate(train_ds):\n",
    "      #sparsity_batch = np.array([])\n",
    "      #Verify if the model needs to be pruned\n",
    "      n_zeros = 0\n",
    "      size = 0\n",
    "\n",
    "      #pruning\n",
    "      if alpha > 0:\n",
    "        bk = []\n",
    "        for layer_weights in model.trainable_variables:\n",
    "          if 'bn' in layer_weights.name:\n",
    "              bk.append(-1)\n",
    "          else:\n",
    "              #flatten weights\n",
    "              f_weights = tf.reshape(layer_weights,[-1])\n",
    "              #get standard deviation of each layer\n",
    "              lim = alpha*tf.math.reduce_std(f_weights)\n",
    "              bk.append(lim)\n",
    "              #create a mask\n",
    "              mask = tf.cast(tf.where(tf.abs(layer_weights)>lim,1,0), tf.float32)\n",
    "              #assign pruned weights to the layer\n",
    "              layer_weights.assign(tf.math.multiply(layer_weights,mask))\n",
    "              #check sparsity\n",
    "              flat_array = np.array((tf.reshape(mask,[-1])))\n",
    "              n_zeros += np.count_nonzero(np.array(flat_array) == 0)\n",
    "              #n_zeros = np.count_nonzero(m)\n",
    "              size += flat_array.shape[0]\n",
    "              #print(size)\n",
    "              #print(n_zeros)\n",
    "              sparsity = n_zeros*100/size\n",
    "      else:\n",
    "        bk = [0] * len(model.trainable_weights)\n",
    "\n",
    "      #Cópia do modelo\n",
    "      model_copy = keras.models.clone_model(model)\n",
    "      model_copy.set_weights(model.get_weights())\n",
    "      \n",
    "      #Quantização\n",
    "      if n_bits > 0:\n",
    "          for i, layer_weights in enumerate(model.trainable_variables):\n",
    "              #print(layer_weights.name)\n",
    "              if 'bn' in layer_weights.name:\n",
    "                  pass\n",
    "              else:\n",
    "                  qk_line = (tf.reduce_max(tf.math.abs(layer_weights)) - bk[i]) / (2 ** (n_bits - 1) - 1)\n",
    "                  ck = tf.math.round(layer_weights / qk_line) * qk_line\n",
    "                  layer_weights.assign(ck)\n",
    "\n",
    "      with tf.GradientTape() as tape:\n",
    "        pred = model(x_batch_train, training=True)\n",
    "        loss = loss_fn(y_batch_train, pred)\n",
    "        \n",
    "      grads = tape.gradient(loss, model.trainable_weights)\n",
    "\n",
    "      for i, (layer_weights, copied_weights) in enumerate(zip(model.trainable_variables, model_copy.trainable_variables)):\n",
    "              grads[i] = grads[i] * learning_rate\n",
    "              # WEIGHT UPDATE\n",
    "              layer_weights.assign(tf.math.subtract(copied_weights, grads[i]))\n",
    "      \n",
    "      #optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "\n",
    "      predictions = tf.argmax(pred, axis=1, output_type=tf.int32)\n",
    "      acc = train_accuracy(y_batch_train, predictions)\n",
    "\n",
    "      acc_epoch_mean = np.append(acc_epoch_mean, acc)\n",
    "      loss_epoch_mean = np.append(loss_epoch_mean, loss)\n",
    "\n",
    "    if alpha > 0:\n",
    "        bk = []\n",
    "        for layer_weights in model.trainable_variables:\n",
    "            if 'bn' in layer_weights.name:\n",
    "                bk.append(-1)\n",
    "            else:\n",
    "                #flatten weights\n",
    "                f_weights = tf.reshape(layer_weights,[-1])\n",
    "                #get standard deviation of each layer\n",
    "                lim = alpha*tf.math.reduce_std(f_weights)\n",
    "                bk.append(lim)\n",
    "                #create a mask\n",
    "                mask = tf.cast(tf.where(tf.abs(layer_weights)>lim,1,0), tf.float32)\n",
    "                #assign pruned weights to the layer\n",
    "                layer_weights.assign(tf.math.multiply(layer_weights,mask))\n",
    "                #check sparsity\n",
    "                flat_array = np.array((tf.reshape(mask,[-1])))\n",
    "                n_zeros += np.count_nonzero(np.array(flat_array) == 0)\n",
    "                #n_zeros = np.count_nonzero(m)\n",
    "                size += flat_array.shape[0]\n",
    "                #print(size)\n",
    "                #print(n_zeros)\n",
    "                sparsity = n_zeros*100/size\n",
    "    else:\n",
    "        bk = [0] * len(model.trainable_weights)\n",
    "\n",
    "    #Quantização\n",
    "    if n_bits > 0:\n",
    "        for i, layer_weights in enumerate(model.trainable_variables):\n",
    "            if 'bn' in layer_weights.name:\n",
    "                pass\n",
    "            else:\n",
    "                qk_line = (tf.reduce_max(tf.math.abs(layer_weights)) - bk[i]) / (2 ** (n_bits - 1) - 1)\n",
    "                ck = tf.math.round(layer_weights / qk_line) * qk_line\n",
    "                layer_weights.assign(ck)\n",
    "    \n",
    "    bk.clear()\n",
    "    \n",
    "    #Test\n",
    "    for step, (x_batch_test, y_batch_test) in enumerate(test_ds):\n",
    "      test_pred = model(x_batch_test, training=False)\n",
    "      test_loss = loss_fn(y_batch_test,test_pred)\n",
    "      test_prediction = tf.argmax(test_pred, axis=1, output_type=tf.int32)\n",
    "      test_acc = test_accuracy(y_batch_test, test_prediction)\n",
    "\n",
    "      acct_epoch_mean = np.append(acc_epoch_mean, test_acc)\n",
    "      losst_epoch_mean = np.append(loss_epoch_mean, test_loss)\n",
    "\n",
    "    #save mean results on lists\n",
    "    model_test_acc = np.append(model_test_acc, np.mean(acct_epoch_mean)*100)\n",
    "    model_test_loss = np.append(model_test_loss,np.mean(losst_epoch_mean)*100)\n",
    "\n",
    "    model_sparsity = np.append(model_sparsity,sparsity)\n",
    "\n",
    "    model_train_acc = np.append(model_train_acc, np.mean(acc_epoch_mean))\n",
    "    model_train_loss = np.append(model_train_loss,np.mean(loss_epoch_mean))\n",
    "    \n",
    "    print(\"Epoch {}/{} \\t Loss = {:.3f} \\t Train Acc = {:.3f}% \\t Sparsity = {:.3f}% \\t Test Acc = {:.3f}%\".format(epoch+1,epochs,float(loss),float(acc*100),sparsity,float(test_acc*100)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len = len(model.trainable_weights)\n",
    "for i in range(len):\n",
    "  a = tf.reshape(model.trainable_weights[i],[-1])\n",
    "  b = a.numpy()\n",
    "  #print(a)\n",
    "  #plt.ylim(0,300)\n",
    "  plt.title(str(i))\n",
    "  plt.hist(b,200)\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9deed99bff0d090d12528913ea8292546286ae8cc7310d67c7e264c4452f2e1b"
  },
  "kernelspec": {
   "display_name": "Python 3.7.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
