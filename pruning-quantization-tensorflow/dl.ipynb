{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"dl.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"th505qHOZFat","executionInfo":{"status":"ok","timestamp":1637790284838,"user_tz":180,"elapsed":2725,"user":{"displayName":"Mateus Goldbarg","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00301443075744424772"}},"outputId":"c3de0faf-f28a-4712-d47b-d41f0e3d3f77"},"source":["import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras import layers\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from matplotlib.pyplot import figure\n","print(tf.__version__)"],"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["2.7.0\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"s_qgeVAPbdzh","executionInfo":{"status":"ok","timestamp":1637790286578,"user_tz":180,"elapsed":259,"user":{"displayName":"Mateus Goldbarg","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00301443075744424772"}},"outputId":"a7e7e08a-6e29-4027-d21e-cdc3662aab0b"},"source":["print(len(tf.config.experimental.list_physical_devices('GPU')))"],"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["1\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mkF6brtrZH-J","executionInfo":{"status":"ok","timestamp":1637790297975,"user_tz":180,"elapsed":10352,"user":{"displayName":"Mateus Goldbarg","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00301443075744424772"}},"outputId":"643b02dd-9ce8-4318-c866-fbecf20571ba"},"source":["\n","#IMPORTAÇÃO E NORRMALIZAÇÃO\n","(x_train, y_train), (x_test,y_test) = keras.datasets.cifar10.load_data()\n","\n","x_train = x_train.astype(float)/255\n","x_test = x_test.astype(float)/255\n","\n","#CRIAR DATASET\n","train_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(50000).batch(64)\n","test_ds = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(64)"],"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n","170500096/170498071 [==============================] - 3s 0us/step\n","170508288/170498071 [==============================] - 3s 0us/step\n"]}]},{"cell_type":"code","metadata":{"id":"A_IZC8boZS8h","executionInfo":{"status":"ok","timestamp":1637790298358,"user_tz":180,"elapsed":387,"user":{"displayName":"Mateus Goldbarg","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00301443075744424772"}}},"source":["class Compression:\n","    def __init__(self,model):\n","        self.loss_fn = tf.keras.losses.SparseCategoricalCrossentropy()\n","        self.optimizer = tf.keras.optimizers.SGD(0.01,0,False)\n","        self.acc_metric = keras.metrics.SparseCategoricalAccuracy()\n","        self.test_accuracy = tf.keras.metrics.Accuracy()\n","        self.test_loss_fn = tf.keras.losses.SparseCategoricalCrossentropy()\n","        self.train_accuracy = tf.keras.metrics.Accuracy()\n","        self.model = model\n","\n","    def train(self,epochs,train_ds,test_ds, alpha, n_bits,learning_rate):\n","        self.train_ds = train_ds\n","        self.test_ds = test_ds\n","        self.epochs = epochs\n","        self.alpha = alpha\n","        self.n_bits = n_bits\n","        self.learning_rate = learning_rate\n","\n","        np.set_printoptions(threshold=np.inf)\n","        model_sparsity = np.array([])\n","        model_sparsity_layers = np.array([])\n","        model_train_loss = np.array([])\n","        model_train_acc = np.array([])\n","        model_test_loss = np.array([])\n","        model_test_acc = np.array([])\n","        sparsity = 0\n","\n","        #CONVERT BIAS OF ZEROS TO ONES\n","        for layer in self.model.trainable_weights:\n","            if 'bias' in layer.name:\n","                new_bias = tf.cast(tf.where(tf.abs(layer) > 0, 1,1), tf.float32)\n","                layer.assign(new_bias)\n","\n","\n","        for epoch in range(self.epochs):\n","            #print(\"\\n---------------------Start of epoch %d------------------\" % (epoch,))\n","            # Iterate over the batches of the dataset.\n","            acc_epoch_mean = np.array([])\n","            loss_epoch_mean = np.array([])\n","\n","            acct_epoch_mean = np.array([])\n","            losst_epoch_mean = np.array([])\n","            for step, (x_batch_train, y_batch_train) in enumerate(self.train_ds):\n","                \n","\n","                #print(\"Depois - depois\")\n","                #print(self.model.trainable_weights[0][0])\n","\n","                #PRUNING\n","                if self.alpha <= 0:\n","                    bk = [0]*len(self.model.trainable_weights)\n","                    print(\"menor\")\n","                else:\n","                    bk,sparsity = self.pruning()\n","\n","                \n","\n","\n","\n","                '''\n","                if self.alpha <=0:\n","                    bk = [0]*len(self.model.trainable_weights)\n","                else:\n","                    bk = []\n","                #PRUNING\n","                if self.alpha > 0:\n","                    for layer_weights in self.model.trainable_weights:\n","                        # flatten weights\n","                        f_weights = tf.reshape(layer_weights, [-1])\n","                        # get standard deviation of each layer\n","                        lim = self.alpha * tf.math.reduce_std(f_weights)\n","                        bk.append(lim)\n","                        # create a mask\n","                        mask = tf.cast(tf.where(tf.abs(layer_weights) > lim, 1, 0), tf.float32)\n","                        # assign pruned weights to the layer\n","                        layer_weights.assign(tf.math.multiply(layer_weights, mask))\n","                        flat_array = np.array((tf.reshape(mask, [-1])))\n","                        n_zeros += np.count_nonzero(np.array(flat_array) == 0)\n","                        size += flat_array.shape[0]\n","                        sparsity = n_zeros * 100 / size\n","                '''\n","\n","                #CREATE A COPY OF THE MODEL TO P->Q\n","                model_copy = keras.models.clone_model(self.model)\n","                model_copy.set_weights(self.model.get_weights())\n","\n","                #QUANTIZATION\n","\n","\n","                if self.n_bits > 0:\n","                    self.quantization(bk)\n","\n","\n","                '''\n","                if step == -500 and epoch == -100:\n","                    pass\n","                else:\n","                    if self.n_bits > 0:\n","                        for i, layer_weights in enumerate(self.model.trainable_variables):\n","                            qk_line = (tf.reduce_max(tf.math.abs(layer_weights))-bk[i]) / (2 ** (self.n_bits - 1) - 1)\n","                            ck = tf.math.round(layer_weights / qk_line) * qk_line\n","                            layer_weights.assign(ck)\n","                '''\n","\n","                tape = tf.GradientTape(persistent=True)\n","                tape._push_tape()\n","                pred = self.model(x_batch_train, training=False)\n","                loss = self.loss_fn(y_batch_train, pred)\n","                tape._pop_tape()\n","\n","                grads = tape.gradient(loss, self.model.trainable_weights)\n","                tape._tape=None\n","\n","                #OPTIMIZER\n","                if self.n_bits > 0 and self.alpha > 0:\n","                    for i, (layer_weights, pruned) in enumerate(zip(self.model.trainable_weights, model_copy.trainable_weights)):\n","                        grads[i] = grads[i] * self.learning_rate\n","                        #WEIGHT UPDATE\n","                        layer_weights.assign(tf.math.subtract(pruned,grads[i]))\n","                        \n","                elif self.alpha <= 0:\n","                    for i, (layer_weights, pruned) in enumerate(zip(self.model.trainable_weights, model_copy.trainable_weights)):\n","                        grads[i] = grads[i] * self.learning_rate\n","                        #WEIGHT UPDATE\n","                        layer_weights.assign(tf.math.subtract(layer_weights,grads[i]))\n","                \n","                predictions = tf.argmax(pred, axis=1, output_type=tf.int32)\n","                acc = self.train_accuracy(y_batch_train, predictions)\n","\n","                acc_epoch_mean = np.append(acc_epoch_mean, acc)\n","                loss_epoch_mean = np.append(loss_epoch_mean, loss)\n","\n","            #PRUNING\n","            if self.alpha <=0:\n","                bk = [0]*len(self.model.trainable_weights)\n","            else:\n","                bk,sparsity = self.pruning()\n","                \n","            #QUANTIZATION\n","            if self.n_bits > 0:\n","                self.quantization(bk)\n","            '''\n","            #PLOT WEIGHTS\n","            a = tf.reshape(model_cifar.trainable_weights[2],[-1])\n","            #print(a)\n","            plt.ylim(0,4000)\n","            plt.hist(a,100)\n","            plt.show()\n","            '''    \n","\n","\n","                #if step % 200 == 0:\n","                    #print(\"So far\", step, \"m_batches ->\", \"train loss =\", float(loss), \"train acc =\", float(acc * 100), \"sparsity =\", sparsity)\n","\n","            # Validation\n","            for step, (x_batch_test, y_batch_test) in enumerate(self.test_ds):\n","                test_pred = self.model(x_batch_test, training=False)\n","                test_loss = self.loss_fn(y_batch_test, test_pred)\n","                test_prediction = tf.argmax(test_pred, axis=1, output_type=tf.int32)\n","                test_acc = self.test_accuracy(y_batch_test, test_prediction)\n","\n","                acct_epoch_mean = np.append(acc_epoch_mean, test_acc)\n","                losst_epoch_mean = np.append(loss_epoch_mean, test_loss)\n","                #if step == 156:\n","                #    print(\"Last train values over the epoch:\", epoch)\n","            #print(\"Epoch \" + str(epoch+1) + \"/\" + str(self.epochs) + \"\\t train loss = \" + str(float(loss)) + \"\\t train acc = \" + str(float(acc * 100)) +\"%\" + \"\\tsparsity = \" + str(sparsity) + \"\\ttest loss = \" + str(float(test_loss)), \"\\ttest accuracy = \" + str(float(test_acc*100)))\n","            print(\"Epoch {}/{} \\t Train Loss = {:.4f} \\t Train Acc = {:.4f}% \\t Sparsity = {:.4f}% \\t Test Loss = {:.4f} \\t Test Acc = {:.4f}%\".format(epoch+1,self.epochs,float(loss),float(acc*100),sparsity,float(test_loss),float(test_acc*100)))\n","\n","            # save mean results on lists\n","            model_test_acc = np.append(model_test_acc, np.mean(acct_epoch_mean) * 100)\n","            model_test_loss = np.append(model_test_loss, np.mean(losst_epoch_mean) * 100)\n","\n","            model_sparsity = np.append(model_sparsity, sparsity)\n","\n","            model_train_acc = np.append(model_train_acc, np.mean(acc_epoch_mean))\n","            model_train_loss = np.append(model_train_loss, np.mean(loss_epoch_mean))\n","\n","    def pruning(self):\n","        bk = []\n","        n_zeros = 0\n","        size = 0\n","        for layer_weights in self.model.trainable_weights:\n","            # flatten weights\n","            f_weights = tf.reshape(layer_weights, [-1])\n","            # get standard deviation of each layer\n","            lim = self.alpha * tf.math.reduce_std(f_weights)\n","            bk.append(float(lim))\n","            # create a mask\n","            mask = tf.cast(tf.where(tf.abs(layer_weights) > lim, 1, 0), tf.float32)\n","            # assign pruned weights to the layer\n","            layer_weights.assign(tf.math.multiply(layer_weights, mask))\n","            flat_array = np.array((tf.reshape(mask, [-1])))\n","            n_zeros += np.count_nonzero(np.array(flat_array) == 0)\n","            size += flat_array.shape[0]\n","        sparsity = n_zeros * 100 / size\n","        return bk, sparsity\n","\n","\n","    def quantization(self,bk):\n","        #print(bk)\n","        for i, layer_weights in enumerate(self.model.trainable_variables):\n","            #print(layer_weights[0])\n","            qk_line = (tf.reduce_max(tf.math.abs(layer_weights))-bk[i]) / (2 ** (self.n_bits - 1) - 1)\n","            #print(float(qk_line))\n","            ck = tf.math.round(layer_weights / qk_line) * qk_line\n","            layer_weights.assign(ck)\n","\n","\n","    def plot_weights(self,layer,lim_x=[0,0],lim_y=0,bins=250):\n","        for layer in model.layers:\n","            if layer.name == layer:\n","                a = tf.reshape(layer.weights[0], [-1])\n","                figure(figsize=(10, 7), dpi=80)\n","                if lim_x != [0,0]:\n","                    plt.xlim((lim_x[0],lim_x[1]))\n","                if lim_y != [0,0]:\n","                    plt.ylim((lim_y[0],lim_y[1]))\n","                plt.hist(a, bins)\n","                plt.show()\n","\n","    def test(self, model, x_test,y_test):\n","        test_accuracy = tf.keras.metrics.Accuracy()\n","        logits = self.model(x_test, training=False)\n","        prediction = tf.argmax(logits, axis=1, output_type=tf.int32)\n","        self.test_accuracy(prediction, y_test)\n","        print(\"Test set accuracy: {:.3%}\".format(test_accuracy.result()))"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sm7mtGmXZKb6","outputId":"91e7b78f-e15a-4fea-cc5e-a859fce048df"},"source":["model_cifar = tf.keras.models.Sequential([\n","  keras.layers.InputLayer(input_shape=(32, 32,3),batch_size=64),\n","     \n","        keras.layers.Conv2D(32, (3, 3), name=\"conv1\", activation='relu',padding='same'),\n","        keras.layers.Conv2D(32, (3, 3), name=\"conv2\", activation='relu',padding='same'),\n","        keras.layers.MaxPooling2D(pool_size=(2, 2),name=\"mp1\"),\n","\n","        keras.layers.Conv2D(64, (3, 3),name=\"conv3\", activation='relu',padding='same'),\n","        keras.layers.Conv2D(64, (3, 3),name=\"conv4\", activation='relu',padding='same'),\n","        keras.layers.MaxPooling2D(pool_size=(2, 2),name=\"mp2\"),\n","\n","        keras.layers.Conv2D(128, (3, 3),name=\"conv5\", activation='relu',padding='same'),\n","        keras.layers.Conv2D(128, (3, 3),name=\"conv6\", activation='relu',padding='same'),\n","        keras.layers.MaxPooling2D(pool_size=(2, 2),name=\"mp3\"),\n","\n","     \n","        keras.layers.Flatten(name = \"flatten\"),\n","     \n","        keras.layers.Dense(256, name=\"dense2\", activation='relu'),\n","        #keras.layers.Dropout(0.2),\n","        keras.layers.Dense(128, name=\"dense3\", activation='relu'),\n","        keras.layers.Dense(10, name=\"dense4\", activation='softmax')\n","])\n","model_cifar.summary()\n","\n","train_compression_cifar = Compression(model_cifar)\n","\n","#.train(n_epochs, train dataset, teste dataset, alpha, n_bits, learning rate)\n","train_compression_cifar.train(80,train_ds,test_ds,0.5,4,0.025)\n"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," conv1 (Conv2D)              (64, 32, 32, 32)          896       \n","                                                                 \n"," conv2 (Conv2D)              (64, 32, 32, 32)          9248      \n","                                                                 \n"," mp1 (MaxPooling2D)          (64, 16, 16, 32)          0         \n","                                                                 \n"," conv3 (Conv2D)              (64, 16, 16, 64)          18496     \n","                                                                 \n"," conv4 (Conv2D)              (64, 16, 16, 64)          36928     \n","                                                                 \n"," mp2 (MaxPooling2D)          (64, 8, 8, 64)            0         \n","                                                                 \n"," conv5 (Conv2D)              (64, 8, 8, 128)           73856     \n","                                                                 \n"," conv6 (Conv2D)              (64, 8, 8, 128)           147584    \n","                                                                 \n"," mp3 (MaxPooling2D)          (64, 4, 4, 128)           0         \n","                                                                 \n"," flatten (Flatten)           (64, 2048)                0         \n","                                                                 \n"," dense2 (Dense)              (64, 256)                 524544    \n","                                                                 \n"," dense3 (Dense)              (64, 128)                 32896     \n","                                                                 \n"," dense4 (Dense)              (64, 10)                  1290      \n","                                                                 \n","=================================================================\n","Total params: 845,738\n","Trainable params: 845,738\n","Non-trainable params: 0\n","_________________________________________________________________\n","Epoch 1/80 \t Train Loss = 2.0476 \t Train Acc = 15.1060% \t Sparsity = 31.7057% \t Test Loss = 1.9717 \t Test Acc = 25.9200%\n","Epoch 2/80 \t Train Loss = 1.7919 \t Train Acc = 23.7040% \t Sparsity = 38.0735% \t Test Loss = 1.5425 \t Test Acc = 29.9350%\n","Epoch 3/80 \t Train Loss = 1.7840 \t Train Acc = 29.5307% \t Sparsity = 38.6594% \t Test Loss = 1.5896 \t Test Acc = 33.4667%\n","Epoch 4/80 \t Train Loss = 1.2065 \t Train Acc = 34.1290% \t Sparsity = 39.5762% \t Test Loss = 0.9843 \t Test Acc = 37.2500%\n","Epoch 5/80 \t Train Loss = 1.4243 \t Train Acc = 37.7128% \t Sparsity = 40.0890% \t Test Loss = 1.2160 \t Test Acc = 39.1980%\n","Epoch 6/80 \t Train Loss = 1.2265 \t Train Acc = 40.5777% \t Sparsity = 40.5569% \t Test Loss = 1.0351 \t Test Acc = 41.1383%\n","Epoch 7/80 \t Train Loss = 1.3833 \t Train Acc = 42.9423% \t Sparsity = 40.8755% \t Test Loss = 1.1538 \t Test Acc = 42.5143%\n","Epoch 8/80 \t Train Loss = 1.8573 \t Train Acc = 44.9522% \t Sparsity = 41.2873% \t Test Loss = 0.8084 \t Test Acc = 43.6950%\n","Epoch 9/80 \t Train Loss = 1.1262 \t Train Acc = 46.7091% \t Sparsity = 41.6655% \t Test Loss = 1.4960 \t Test Acc = 44.3856%\n","Epoch 10/80 \t Train Loss = 0.7317 \t Train Acc = 48.2866% \t Sparsity = 41.9300% \t Test Loss = 0.8828 \t Test Acc = 45.3140%\n","Epoch 11/80 \t Train Loss = 1.1923 \t Train Acc = 49.7080% \t Sparsity = 42.1916% \t Test Loss = 1.0761 \t Test Acc = 46.2145%\n","Epoch 12/80 \t Train Loss = 0.9247 \t Train Acc = 50.9917% \t Sparsity = 42.3641% \t Test Loss = 1.1037 \t Test Acc = 47.0550%\n","Epoch 13/80 \t Train Loss = 1.6072 \t Train Acc = 52.1237% \t Sparsity = 42.5091% \t Test Loss = 1.0604 \t Test Acc = 47.8023%\n","Epoch 14/80 \t Train Loss = 1.6008 \t Train Acc = 53.1690% \t Sparsity = 42.6486% \t Test Loss = 1.1235 \t Test Acc = 48.2229%\n","Epoch 15/80 \t Train Loss = 0.9970 \t Train Acc = 54.0819% \t Sparsity = 42.8627% \t Test Loss = 1.2327 \t Test Acc = 48.7547%\n","Epoch 16/80 \t Train Loss = 1.0130 \t Train Acc = 54.9519% \t Sparsity = 43.1316% \t Test Loss = 1.3638 \t Test Acc = 49.2375%\n","Epoch 17/80 \t Train Loss = 0.8148 \t Train Acc = 55.7624% \t Sparsity = 43.3310% \t Test Loss = 1.4860 \t Test Acc = 49.7859%\n","Epoch 18/80 \t Train Loss = 1.2761 \t Train Acc = 56.5814% \t Sparsity = 43.5194% \t Test Loss = 1.4603 \t Test Acc = 50.1339%\n","Epoch 19/80 \t Train Loss = 1.0218 \t Train Acc = 57.2762% \t Sparsity = 43.6985% \t Test Loss = 1.3090 \t Test Acc = 50.2063%\n","Epoch 20/80 \t Train Loss = 0.5321 \t Train Acc = 57.9472% \t Sparsity = 43.8541% \t Test Loss = 1.3174 \t Test Acc = 50.5940%\n","Epoch 21/80 \t Train Loss = 1.0571 \t Train Acc = 58.5188% \t Sparsity = 44.0018% \t Test Loss = 1.4526 \t Test Acc = 50.8224%\n","Epoch 22/80 \t Train Loss = 0.7492 \t Train Acc = 59.0928% \t Sparsity = 44.1485% \t Test Loss = 1.1020 \t Test Acc = 51.2336%\n","Epoch 23/80 \t Train Loss = 1.0470 \t Train Acc = 59.6302% \t Sparsity = 44.2492% \t Test Loss = 1.5219 \t Test Acc = 51.4735%\n","Epoch 24/80 \t Train Loss = 0.9909 \t Train Acc = 60.0960% \t Sparsity = 44.3882% \t Test Loss = 1.7376 \t Test Acc = 51.6963%\n","Epoch 25/80 \t Train Loss = 0.9665 \t Train Acc = 60.5370% \t Sparsity = 44.5144% \t Test Loss = 1.9224 \t Test Acc = 51.8204%\n","Epoch 26/80 \t Train Loss = 1.0495 \t Train Acc = 60.9482% \t Sparsity = 44.6279% \t Test Loss = 0.9037 \t Test Acc = 52.1438%\n","Epoch 27/80 \t Train Loss = 1.1175 \t Train Acc = 61.3440% \t Sparsity = 44.7361% \t Test Loss = 1.2544 \t Test Acc = 52.2681%\n","Epoch 28/80 \t Train Loss = 0.7855 \t Train Acc = 61.6884% \t Sparsity = 44.8433% \t Test Loss = 1.1893 \t Test Acc = 52.4439%\n","Epoch 29/80 \t Train Loss = 1.2064 \t Train Acc = 61.9961% \t Sparsity = 44.9567% \t Test Loss = 1.5786 \t Test Acc = 52.6483%\n","Epoch 30/80 \t Train Loss = 1.3709 \t Train Acc = 62.2897% \t Sparsity = 45.0507% \t Test Loss = 0.9329 \t Test Acc = 52.8293%\n","Epoch 31/80 \t Train Loss = 0.6021 \t Train Acc = 62.5625% \t Sparsity = 45.1413% \t Test Loss = 1.2873 \t Test Acc = 52.9781%\n","Epoch 32/80 \t Train Loss = 0.7640 \t Train Acc = 62.8110% \t Sparsity = 45.2323% \t Test Loss = 1.1787 \t Test Acc = 53.0516%\n","Epoch 33/80 \t Train Loss = 0.4387 \t Train Acc = 63.0461% \t Sparsity = 45.3255% \t Test Loss = 1.5078 \t Test Acc = 53.2585%\n","Epoch 34/80 \t Train Loss = 0.7198 \t Train Acc = 63.2766% \t Sparsity = 45.4061% \t Test Loss = 1.2188 \t Test Acc = 53.3944%\n","Epoch 35/80 \t Train Loss = 0.9682 \t Train Acc = 63.4946% \t Sparsity = 45.4949% \t Test Loss = 1.4174 \t Test Acc = 53.5494%\n","Epoch 36/80 \t Train Loss = 1.1479 \t Train Acc = 63.6986% \t Sparsity = 45.5647% \t Test Loss = 1.1986 \t Test Acc = 53.6825%\n","Epoch 37/80 \t Train Loss = 0.8849 \t Train Acc = 63.8658% \t Sparsity = 45.6366% \t Test Loss = 1.3133 \t Test Acc = 53.7578%\n","Epoch 38/80 \t Train Loss = 1.0013 \t Train Acc = 63.9999% \t Sparsity = 45.7520% \t Test Loss = 1.8750 \t Test Acc = 53.7784%\n","Epoch 39/80 \t Train Loss = 1.0555 \t Train Acc = 64.1261% \t Sparsity = 45.8501% \t Test Loss = 1.5811 \t Test Acc = 53.9069%\n","Epoch 40/80 \t Train Loss = 0.4232 \t Train Acc = 64.2622% \t Sparsity = 45.9264% \t Test Loss = 1.1204 \t Test Acc = 54.0423%\n","Epoch 41/80 \t Train Loss = 0.3837 \t Train Acc = 64.3840% \t Sparsity = 45.9809% \t Test Loss = 1.1392 \t Test Acc = 54.1732%\n","Epoch 42/80 \t Train Loss = 0.4775 \t Train Acc = 64.4967% \t Sparsity = 46.0353% \t Test Loss = 0.7756 \t Test Acc = 54.2850%\n","Epoch 43/80 \t Train Loss = 0.8141 \t Train Acc = 64.5948% \t Sparsity = 46.0861% \t Test Loss = 1.9903 \t Test Acc = 54.3321%\n","Epoch 44/80 \t Train Loss = 1.4313 \t Train Acc = 64.6719% \t Sparsity = 46.1486% \t Test Loss = 1.3081 \t Test Acc = 54.4107%\n","Epoch 45/80 \t Train Loss = 1.2692 \t Train Acc = 64.7320% \t Sparsity = 46.2056% \t Test Loss = 1.0658 \t Test Acc = 54.5071%\n","Epoch 46/80 \t Train Loss = 0.8673 \t Train Acc = 64.7821% \t Sparsity = 46.2438% \t Test Loss = 1.3241 \t Test Acc = 54.5578%\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":269},"id":"NGIWZ_ZRZa5J","executionInfo":{"status":"ok","timestamp":1637786764351,"user_tz":180,"elapsed":791,"user":{"displayName":"Mateus Goldbarg","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"00301443075744424772"}},"outputId":"6927b545-ae95-4957-d1c0-c45aa9144a84"},"source":["a = tf.reshape(model_cifar.trainable_weights[2],[-1])\n","#print(a)\n","plt.ylim(0,4000)\n","plt.hist(a,100)\n","plt.show()"],"execution_count":11,"outputs":[{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"needs_background":"light"}}]},{"cell_type":"code","metadata":{"id":"LPqZ_X5pZc-5"},"source":[""],"execution_count":null,"outputs":[]}]}