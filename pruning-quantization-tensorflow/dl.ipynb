{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "dl.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "th505qHOZFat",
        "outputId": "13e2ca74-8001-4391-cb39-a6c47518cb3c"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.pyplot import figure\n",
        "print(tf.__version__)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.7.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s_qgeVAPbdzh",
        "outputId": "4cda2789-98c0-4c93-e01d-edb3893bf0bd"
      },
      "source": [
        "print(len(tf.config.experimental.list_physical_devices('GPU')))"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mkF6brtrZH-J"
      },
      "source": [
        "\n",
        "#IMPORTAÇÃO E NORRMALIZAÇÃO\n",
        "(x_train, y_train), (x_test,y_test) = keras.datasets.mnist.load_data()\n",
        "\n",
        "x_train = x_train.astype(float)/255\n",
        "x_test = x_test.astype(float)/255\n",
        "\n",
        "#CRIAR DATASET\n",
        "train_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(50000).batch(64)\n",
        "test_ds = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(64)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A_IZC8boZS8h"
      },
      "source": [
        "class Compression:\n",
        "    def __init__(self,model):\n",
        "        self.loss_fn = tf.keras.losses.SparseCategoricalCrossentropy()\n",
        "        self.optimizer = tf.keras.optimizers.SGD(0.01,0,False)\n",
        "        self.acc_metric = keras.metrics.SparseCategoricalAccuracy()\n",
        "        self.test_accuracy = tf.keras.metrics.Accuracy()\n",
        "        self.test_loss_fn = tf.keras.losses.SparseCategoricalCrossentropy()\n",
        "        self.train_accuracy = tf.keras.metrics.Accuracy()\n",
        "        self.model = model\n",
        "\n",
        "    def train(self,epochs,train_ds,test_ds, alpha, n_bits,learning_rate):\n",
        "        self.train_ds = train_ds\n",
        "        self.test_ds = test_ds\n",
        "        self.epochs = epochs\n",
        "        self.alpha = alpha\n",
        "        self.n_bits = n_bits\n",
        "        self.learning_rate = learning_rate\n",
        "\n",
        "        np.set_printoptions(threshold=np.inf)\n",
        "        model_sparsity = np.array([])\n",
        "        model_sparsity_layers = np.array([])\n",
        "        model_train_loss = np.array([])\n",
        "        model_train_acc = np.array([])\n",
        "        model_test_loss = np.array([])\n",
        "        model_test_acc = np.array([])\n",
        "        sparsity = 0\n",
        "\n",
        "        #CONVERT BIAS OF ZEROS TO ONES\n",
        "        for layer in self.model.trainable_weights:\n",
        "            if 'bias' in layer.name:\n",
        "                new_bias = tf.cast(tf.where(tf.abs(layer) > 0, 1,1), tf.float32)\n",
        "                layer.assign(new_bias)\n",
        "\n",
        "\n",
        "        for epoch in range(self.epochs):\n",
        "            #print(\"\\n---------------------Start of epoch %d------------------\" % (epoch,))\n",
        "            # Iterate over the batches of the dataset.\n",
        "            acc_epoch_mean = np.array([])\n",
        "            loss_epoch_mean = np.array([])\n",
        "\n",
        "            acct_epoch_mean = np.array([])\n",
        "            losst_epoch_mean = np.array([])\n",
        "            for step, (x_batch_train, y_batch_train) in enumerate(self.train_ds):\n",
        "                \n",
        "\n",
        "                #print(\"Depois - depois\")\n",
        "                #print(self.model.trainable_weights[0][0])\n",
        "\n",
        "                #PRUNING\n",
        "                if self.alpha <= 0:\n",
        "                    bk = [0]*len(self.model.trainable_weights)\n",
        "                    print(\"menor\")\n",
        "                else:\n",
        "                    bk,sparsity = self.pruning()\n",
        "\n",
        "                \n",
        "\n",
        "\n",
        "\n",
        "                '''\n",
        "                if self.alpha <=0:\n",
        "                    bk = [0]*len(self.model.trainable_weights)\n",
        "                else:\n",
        "                    bk = []\n",
        "                #PRUNING\n",
        "                if self.alpha > 0:\n",
        "                    for layer_weights in self.model.trainable_weights:\n",
        "                        # flatten weights\n",
        "                        f_weights = tf.reshape(layer_weights, [-1])\n",
        "                        # get standard deviation of each layer\n",
        "                        lim = self.alpha * tf.math.reduce_std(f_weights)\n",
        "                        bk.append(lim)\n",
        "                        # create a mask\n",
        "                        mask = tf.cast(tf.where(tf.abs(layer_weights) > lim, 1, 0), tf.float32)\n",
        "                        # assign pruned weights to the layer\n",
        "                        layer_weights.assign(tf.math.multiply(layer_weights, mask))\n",
        "                        flat_array = np.array((tf.reshape(mask, [-1])))\n",
        "                        n_zeros += np.count_nonzero(np.array(flat_array) == 0)\n",
        "                        size += flat_array.shape[0]\n",
        "                        sparsity = n_zeros * 100 / size\n",
        "                '''\n",
        "\n",
        "                #CREATE A COPY OF THE MODEL TO P->Q\n",
        "                model_copy = keras.models.clone_model(self.model)\n",
        "                model_copy.set_weights(self.model.get_weights())\n",
        "\n",
        "                #QUANTIZATION\n",
        "\n",
        "\n",
        "                if self.n_bits > 0:\n",
        "                    self.quantization(bk)\n",
        "\n",
        "\n",
        "                '''\n",
        "                if step == -500 and epoch == -100:\n",
        "                    pass\n",
        "                else:\n",
        "                    if self.n_bits > 0:\n",
        "                        for i, layer_weights in enumerate(self.model.trainable_variables):\n",
        "                            qk_line = (tf.reduce_max(tf.math.abs(layer_weights))-bk[i]) / (2 ** (self.n_bits - 1) - 1)\n",
        "                            ck = tf.math.round(layer_weights / qk_line) * qk_line\n",
        "                            layer_weights.assign(ck)\n",
        "                '''\n",
        "\n",
        "                tape = tf.GradientTape(persistent=True)\n",
        "                tape._push_tape()\n",
        "                pred = self.model(x_batch_train, training=False)\n",
        "                loss = self.loss_fn(y_batch_train, pred)\n",
        "                tape._pop_tape()\n",
        "\n",
        "                grads = tape.gradient(loss, self.model.trainable_weights)\n",
        "                tape._tape=None\n",
        "\n",
        "                #OPTIMIZER\n",
        "                if self.n_bits > 0 and self.alpha > 0:\n",
        "                    for i, (layer_weights, pruned) in enumerate(zip(self.model.trainable_weights, model_copy.trainable_weights)):\n",
        "                        grads[i] = grads[i] * self.learning_rate\n",
        "                        #WEIGHT UPDATE\n",
        "                        layer_weights.assign(tf.math.subtract(pruned,grads[i]))\n",
        "                        \n",
        "                elif self.alpha <= 0:\n",
        "                    for i, (layer_weights, pruned) in enumerate(zip(self.model.trainable_weights, model_copy.trainable_weights)):\n",
        "                        grads[i] = grads[i] * self.learning_rate\n",
        "                        #WEIGHT UPDATE\n",
        "                        layer_weights.assign(tf.math.subtract(layer_weights,grads[i]))\n",
        "                \n",
        "                predictions = tf.argmax(pred, axis=1, output_type=tf.int32)\n",
        "                acc = self.train_accuracy(y_batch_train, predictions)\n",
        "\n",
        "                acc_epoch_mean = np.append(acc_epoch_mean, acc)\n",
        "                loss_epoch_mean = np.append(loss_epoch_mean, loss)\n",
        "\n",
        "            #PRUNING\n",
        "            if self.alpha <=0:\n",
        "                bk = [0]*len(self.model.trainable_weights)\n",
        "            else:\n",
        "                bk,sparsity = self.pruning()\n",
        "                \n",
        "            #QUANTIZATION\n",
        "            if self.n_bits > 0:\n",
        "                self.quantization(bk)\n",
        "                \n",
        "\n",
        "\n",
        "                #if step % 200 == 0:\n",
        "                    #print(\"So far\", step, \"m_batches ->\", \"train loss =\", float(loss), \"train acc =\", float(acc * 100), \"sparsity =\", sparsity)\n",
        "\n",
        "            # Validation\n",
        "            for step, (x_batch_test, y_batch_test) in enumerate(self.test_ds):\n",
        "                test_pred = self.model(x_batch_test, training=False)\n",
        "                test_loss = self.loss_fn(y_batch_test, test_pred)\n",
        "                test_prediction = tf.argmax(test_pred, axis=1, output_type=tf.int32)\n",
        "                test_acc = self.test_accuracy(y_batch_test, test_prediction)\n",
        "\n",
        "                acct_epoch_mean = np.append(acc_epoch_mean, test_acc)\n",
        "                losst_epoch_mean = np.append(loss_epoch_mean, test_loss)\n",
        "                #if step == 156:\n",
        "                #    print(\"Last train values over the epoch:\", epoch)\n",
        "            #print(\"Epoch \" + str(epoch+1) + \"/\" + str(self.epochs) + \"\\t train loss = \" + str(float(loss)) + \"\\t train acc = \" + str(float(acc * 100)) +\"%\" + \"\\tsparsity = \" + str(sparsity) + \"\\ttest loss = \" + str(float(test_loss)), \"\\ttest accuracy = \" + str(float(test_acc*100)))\n",
        "            print(\"Epoch {}/{} \\t Train Loss = {:.4f} \\t Train Acc = {:.4f}% \\t Sparsity = {:.4f}% \\t Test Loss = {:.4f} \\t Test Acc = {:.4f}%\".format(epoch+1,self.epochs,float(loss),float(acc*100),sparsity,float(test_loss),float(test_acc*100)))\n",
        "\n",
        "            # save mean results on lists\n",
        "            model_test_acc = np.append(model_test_acc, np.mean(acct_epoch_mean) * 100)\n",
        "            model_test_loss = np.append(model_test_loss, np.mean(losst_epoch_mean) * 100)\n",
        "\n",
        "            model_sparsity = np.append(model_sparsity, sparsity)\n",
        "\n",
        "            model_train_acc = np.append(model_train_acc, np.mean(acc_epoch_mean))\n",
        "            model_train_loss = np.append(model_train_loss, np.mean(loss_epoch_mean))\n",
        "\n",
        "    def pruning(self):\n",
        "        bk = []\n",
        "        n_zeros = 0\n",
        "        size = 0\n",
        "        for layer_weights in self.model.trainable_weights:\n",
        "            # flatten weights\n",
        "            f_weights = tf.reshape(layer_weights, [-1])\n",
        "            # get standard deviation of each layer\n",
        "            lim = self.alpha * tf.math.reduce_std(f_weights)\n",
        "            bk.append(float(lim))\n",
        "            # create a mask\n",
        "            mask = tf.cast(tf.where(tf.abs(layer_weights) > lim, 1, 0), tf.float32)\n",
        "            # assign pruned weights to the layer\n",
        "            layer_weights.assign(tf.math.multiply(layer_weights, mask))\n",
        "            flat_array = np.array((tf.reshape(mask, [-1])))\n",
        "            n_zeros += np.count_nonzero(np.array(flat_array) == 0)\n",
        "            size += flat_array.shape[0]\n",
        "        sparsity = n_zeros * 100 / size\n",
        "        return bk, sparsity\n",
        "\n",
        "\n",
        "    def quantization(self,bk):\n",
        "        #print(bk)\n",
        "        for i, layer_weights in enumerate(self.model.trainable_variables):\n",
        "            #print(layer_weights[0])\n",
        "            qk_line = (tf.reduce_max(tf.math.abs(layer_weights))-bk[i]) / (2 ** (self.n_bits - 1) - 1)\n",
        "            #print(float(qk_line))\n",
        "            ck = tf.math.round(layer_weights / qk_line) * qk_line\n",
        "            layer_weights.assign(ck)\n",
        "\n",
        "\n",
        "    def plot_weights(self,layer,lim_x=[0,0],lim_y=0,bins=250):\n",
        "        for layer in model.layers:\n",
        "            if layer.name == layer:\n",
        "                a = tf.reshape(layer.weights[0], [-1])\n",
        "                figure(figsize=(10, 7), dpi=80)\n",
        "                if lim_x != [0,0]:\n",
        "                    plt.xlim((lim_x[0],lim_x[1]))\n",
        "                if lim_y != [0,0]:\n",
        "                    plt.ylim((lim_y[0],lim_y[1]))\n",
        "                plt.hist(a, bins)\n",
        "                plt.show()\n",
        "\n",
        "    def test(self, model, x_test,y_test):\n",
        "        test_accuracy = tf.keras.metrics.Accuracy()\n",
        "        logits = self.model(x_test, training=False)\n",
        "        prediction = tf.argmax(logits, axis=1, output_type=tf.int32)\n",
        "        self.test_accuracy(prediction, y_test)\n",
        "        print(\"Test set accuracy: {:.3%}\".format(test_accuracy.result()))"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sm7mtGmXZKb6",
        "outputId": "0f896ba2-a822-4609-91d7-4d9c6438be7f"
      },
      "source": [
        "model_mnist = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.Flatten(input_shape=(28, 28),name=\"input\",batch_size=64),\n",
        "  tf.keras.layers.Dense(32, activation='relu',name=\"dense1\"),\n",
        "  tf.keras.layers.Dense(32, activation='relu',name=\"dense2\"),\n",
        "  tf.keras.layers.Dense(10 ,activation='softmax',name=\"output\")\n",
        "])\n",
        "model_mnist.summary()\n",
        "\n",
        "train_compression_mnist = Compression(model_mnist)\n",
        "\n",
        "#.train(n_epochs, train dataset, teste dataset, alpha, n_bits, learning rate)\n",
        "train_compression_mnist.train(5,train_ds,test_ds,0.5,4,0.05)\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input (Flatten)             (64, 784)                 0         \n",
            "                                                                 \n",
            " dense1 (Dense)              (64, 32)                  25120     \n",
            "                                                                 \n",
            " dense2 (Dense)              (64, 32)                  1056      \n",
            "                                                                 \n",
            " output (Dense)              (64, 10)                  330       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 26,506\n",
            "Trainable params: 26,506\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/5 \t Train Loss = 0.1844 \t Train Acc = 85.6817% \t Sparsity = 42.3489% \t Test Loss = 0.1448 \t Test Acc = 90.7900%\n",
            "Epoch 2/5 \t Train Loss = 0.3861 \t Train Acc = 88.4575% \t Sparsity = 46.0273% \t Test Loss = 0.2805 \t Test Acc = 90.8700%\n",
            "Epoch 3/5 \t Train Loss = 0.2528 \t Train Acc = 89.8294% \t Sparsity = 47.5477% \t Test Loss = 0.1011 \t Test Acc = 91.6133%\n",
            "Epoch 4/5 \t Train Loss = 0.1525 \t Train Acc = 90.6700% \t Sparsity = 48.4947% \t Test Loss = 0.1064 \t Test Acc = 91.9425%\n",
            "Epoch 5/5 \t Train Loss = 0.2151 \t Train Acc = 91.2603% \t Sparsity = 49.1474% \t Test Loss = 0.1706 \t Test Acc = 92.2000%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "NGIWZ_ZRZa5J",
        "outputId": "5d21a5f4-d6ef-4561-c762-e6ef45cbbe0e"
      },
      "source": [
        "a = tf.reshape(model_mnist.trainable_weights[2],[-1])\n",
        "print(a)\n",
        "plt.ylim(0,100)\n",
        "plt.hist(a,500)\n",
        "plt.show()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[-0.44280115 -0.2656807   0.2656807  -0.17712046  0.2656807   0.35424092\n",
            "  0.17712046 -0.          0.          0.44280115 -0.         -0.17712046\n",
            "  0.         -0.         -0.         -0.2656807  -0.35424092 -0.\n",
            " -0.          0.44280115  0.         -0.          0.44280115 -0.35424092\n",
            "  0.         -0.35424092 -0.2656807  -0.         -0.          0.\n",
            "  0.          0.          0.35424092  0.         -0.         -0.\n",
            " -0.         -0.          0.17712046 -0.2656807   0.         -0.5313614\n",
            " -0.35424092  0.44280115  0.2656807  -0.          0.35424092 -0.5313614\n",
            " -0.         -0.         -0.17712046  0.17712046  0.17712046 -0.\n",
            "  0.35424092  0.          0.          0.         -0.         -0.\n",
            " -0.35424092 -0.          0.2656807  -0.          0.5313614   0.\n",
            "  0.2656807  -0.          0.2656807  -0.35424092  0.2656807  -0.\n",
            "  0.         -0.         -0.          0.          0.          0.\n",
            " -0.          0.44280115 -0.5313614   0.         -0.         -0.\n",
            " -0.44280115 -0.17712046 -0.44280115 -0.35424092  0.         -0.17712046\n",
            "  0.44280115 -0.         -0.35424092 -0.          0.         -0.17712046\n",
            " -0.6199216  -0.35424092 -0.2656807   0.          0.2656807  -0.35424092\n",
            "  0.17712046 -0.          0.         -0.2656807  -0.         -0.2656807\n",
            " -0.17712046 -0.          0.35424092 -0.17712046 -0.         -0.35424092\n",
            "  0.44280115  0.44280115 -0.44280115 -0.         -0.          0.\n",
            " -0.2656807  -0.17712046  0.         -0.         -0.         -0.35424092\n",
            " -0.44280115  0.35424092 -0.35424092 -0.35424092 -0.          0.08856023\n",
            "  0.2656807  -0.          0.2656807   0.2656807  -0.17712046  0.5313614\n",
            " -0.35424092  0.          0.         -0.         -0.44280115  0.\n",
            " -0.35424092  0.         -0.17712046  0.35424092 -0.17712046 -0.\n",
            "  0.2656807   0.17712046 -0.35424092  0.          0.          0.17712046\n",
            " -0.2656807   0.          0.         -0.17712046  0.          0.35424092\n",
            "  0.35424092  0.5313614  -0.5313614   0.2656807   0.         -0.17712046\n",
            "  0.          0.         -0.          0.         -0.2656807   0.2656807\n",
            " -0.         -0.5313614  -0.         -0.44280115  0.         -0.\n",
            "  0.44280115 -0.          0.         -0.          0.35424092 -0.\n",
            "  0.17712046 -0.         -0.35424092  0.2656807   0.17712046  0.35424092\n",
            " -0.08856023  0.35424092 -0.17712046 -0.         -0.35424092  0.08856023\n",
            " -0.5313614  -0.          0.2656807   0.          0.35424092 -0.\n",
            " -0.          0.          0.35424092  0.         -0.17712046  0.\n",
            "  0.08856023  0.44280115  0.17712046  0.2656807  -0.         -0.2656807\n",
            "  0.35424092  0.44280115  0.2656807   0.35424092 -0.         -0.\n",
            " -0.          0.          0.2656807  -0.17712046  0.17712046  0.44280115\n",
            "  0.         -0.2656807  -0.         -0.2656807   0.          0.2656807\n",
            " -0.35424092  0.          0.2656807  -0.          0.          0.\n",
            " -0.2656807   0.17712046  0.35424092  0.17712046  0.         -0.\n",
            "  0.17712046  0.2656807  -0.2656807  -0.17712046  0.6199216  -0.2656807\n",
            " -0.5313614   0.2656807  -0.2656807   0.17712046 -0.17712046  0.\n",
            "  0.08856023 -0.2656807   0.2656807  -0.44280115  0.17712046  0.2656807\n",
            "  0.35424092 -0.          0.35424092  0.          0.         -0.2656807\n",
            " -0.         -0.70848185 -0.          0.35424092 -0.         -0.\n",
            " -0.35424092 -0.         -0.          0.          0.2656807   0.6199216\n",
            " -0.44280115  0.35424092  0.17712046  0.44280115  0.35424092  0.17712046\n",
            "  0.          0.17712046 -0.         -0.44280115  0.2656807  -0.35424092\n",
            "  0.         -0.          0.2656807  -0.         -0.35424092  0.\n",
            "  0.35424092 -0.         -0.          0.44280115  0.5313614   0.2656807\n",
            "  0.35424092  0.35424092 -0.         -0.          0.2656807   0.\n",
            " -0.5313614   0.          0.44280115 -0.          0.44280115 -0.\n",
            "  0.44280115 -0.35424092  0.         -0.         -0.          0.\n",
            "  0.35424092 -0.          0.17712046 -0.         -0.2656807  -0.\n",
            " -0.2656807   0.         -0.17712046 -0.17712046 -0.          0.\n",
            "  0.2656807   0.35424092 -0.         -0.          0.         -0.35424092\n",
            "  0.35424092 -0.44280115 -0.44280115  0.          0.2656807  -0.\n",
            " -0.          0.         -0.2656807   0.44280115  0.2656807  -0.35424092\n",
            " -0.         -0.44280115 -0.          0.35424092  0.2656807   0.2656807\n",
            "  0.         -0.         -0.          0.          0.         -0.\n",
            " -0.          0.5313614  -0.         -0.         -0.         -0.\n",
            " -0.2656807   0.17712046  0.6199216   0.          0.          0.\n",
            " -0.44280115 -0.          0.44280115 -0.2656807   0.          0.\n",
            "  0.2656807  -0.17712046  0.2656807   0.         -0.         -0.\n",
            "  0.35424092 -0.35424092 -0.2656807   0.2656807  -0.          0.\n",
            "  0.44280115 -0.35424092 -0.2656807   0.44280115  0.2656807  -0.\n",
            " -0.         -0.         -0.         -0.         -0.          0.\n",
            "  0.          0.         -0.35424092  0.2656807   0.2656807  -0.\n",
            "  0.          0.         -0.35424092 -0.17712046 -0.2656807  -0.35424092\n",
            "  0.35424092  0.35424092 -0.5313614  -0.          0.          0.35424092\n",
            " -0.         -0.35424092  0.          0.         -0.          0.\n",
            " -0.          0.         -0.44280115 -0.44280115 -0.         -0.\n",
            " -0.35424092  0.2656807   0.         -0.          0.2656807  -0.\n",
            " -0.         -0.35424092  0.          0.17712046 -0.2656807  -0.2656807\n",
            " -0.2656807  -0.2656807  -0.35424092 -0.          0.         -0.\n",
            "  0.44280115  0.         -0.          0.          0.2656807  -0.\n",
            "  0.2656807   0.          0.35424092 -0.44280115 -0.         -0.\n",
            " -0.          0.2656807  -0.          0.2656807   0.2656807   0.\n",
            " -0.         -0.2656807  -0.         -0.5313614   0.          0.\n",
            "  0.          0.          0.17712046 -0.44280115  0.2656807  -0.2656807\n",
            " -0.2656807  -0.2656807   0.          0.2656807  -0.2656807   0.\n",
            "  0.2656807  -0.         -0.2656807   0.17712046 -0.2656807   0.\n",
            " -0.         -0.         -0.          0.35424092 -0.35424092 -0.2656807\n",
            "  0.2656807   0.2656807   0.         -0.         -0.         -0.2656807\n",
            "  0.         -0.          0.         -0.17712046 -0.2656807  -0.2656807\n",
            " -0.          0.35424092  0.         -0.2656807  -0.35424092 -0.44280115\n",
            " -0.          0.          0.         -0.         -0.17712046 -0.17712046\n",
            " -0.         -0.2656807  -0.5313614  -0.5313614  -0.         -0.\n",
            " -0.5313614   0.35424092  0.          0.         -0.44280115 -0.\n",
            " -0.         -0.2656807   0.          0.          0.         -0.2656807\n",
            " -0.         -0.         -0.          0.44280115 -0.17712046 -0.2656807\n",
            "  0.          0.         -0.2656807   0.          0.          0.\n",
            "  0.2656807   0.35424092  0.5313614  -0.17712046 -0.         -0.6199216\n",
            " -0.         -0.35424092  0.          0.35424092  0.         -0.5313614\n",
            "  0.         -0.17712046 -0.         -0.          0.         -0.\n",
            "  0.          0.         -0.          0.35424092  0.35424092 -0.\n",
            " -0.5313614  -0.         -0.17712046 -0.35424092  0.2656807  -0.44280115\n",
            " -0.35424092 -0.35424092 -0.          0.          0.35424092 -0.\n",
            "  0.         -0.17712046 -0.17712046 -0.2656807  -0.17712046  0.2656807\n",
            "  0.2656807   0.17712046 -0.         -0.17712046 -0.         -0.2656807\n",
            " -0.44280115  0.2656807   0.          0.17712046  0.17712046  0.\n",
            "  0.17712046 -0.          0.          0.17712046  0.17712046 -0.5313614\n",
            " -0.          0.         -0.17712046 -0.         -0.         -0.2656807\n",
            " -0.         -0.          0.44280115 -0.         -0.2656807  -0.2656807\n",
            " -0.2656807  -0.35424092  0.35424092 -0.          0.2656807  -0.\n",
            " -0.          0.35424092 -0.35424092  0.35424092  0.         -0.\n",
            " -0.2656807  -0.         -0.17712046 -0.         -0.6199216  -0.35424092\n",
            "  0.          0.2656807   0.          0.          0.17712046 -0.\n",
            " -0.          0.          0.44280115 -0.         -0.17712046 -0.\n",
            "  0.17712046 -0.          0.44280115  0.2656807   0.          0.\n",
            " -0.         -0.         -0.         -0.35424092 -0.44280115  0.\n",
            "  0.2656807  -0.         -0.2656807  -0.17712046 -0.          0.\n",
            " -0.         -0.35424092 -0.          0.17712046 -0.         -0.6199216\n",
            " -0.35424092  0.35424092 -0.          0.         -0.35424092 -0.\n",
            " -0.         -0.          0.         -0.44280115 -0.17712046 -0.\n",
            " -0.         -0.5313614  -0.44280115 -0.2656807  -0.17712046  0.\n",
            " -0.35424092 -0.35424092  0.          0.          0.2656807   0.44280115\n",
            " -0.          0.44280115  0.5313614   0.17712046  0.         -0.\n",
            " -0.          0.          0.35424092 -0.          0.2656807   0.\n",
            " -0.          0.         -0.         -0.         -0.         -0.\n",
            " -0.         -0.35424092  0.         -0.17712046  0.2656807  -0.\n",
            " -0.44280115  0.35424092  0.2656807  -0.35424092  0.44280115  0.\n",
            "  0.2656807   0.44280115 -0.          0.17712046  0.35424092 -0.17712046\n",
            "  0.2656807  -0.35424092 -0.          0.35424092 -0.2656807  -0.\n",
            " -0.2656807   0.         -0.         -0.2656807  -0.         -0.2656807\n",
            " -0.08856023 -0.         -0.          0.          0.          0.35424092\n",
            " -0.35424092 -0.2656807  -0.         -0.35424092  0.2656807  -0.17712046\n",
            " -0.44280115  0.35424092  0.2656807  -0.          0.2656807   0.17712046\n",
            "  0.         -0.2656807   0.          0.5313614  -0.          0.\n",
            "  0.2656807  -0.         -0.          0.          0.44280115  0.\n",
            " -0.         -0.44280115 -0.         -0.         -0.35424092  0.2656807\n",
            "  0.          0.2656807   0.         -0.         -0.          0.17712046\n",
            "  0.35424092  0.17712046  0.          0.          0.17712046  0.44280115\n",
            " -0.35424092 -0.          0.         -0.         -0.35424092 -0.17712046\n",
            "  0.2656807   0.35424092  0.         -0.          0.2656807   0.35424092\n",
            " -0.         -0.44280115 -0.35424092 -0.         -0.17712046 -0.\n",
            " -0.         -0.2656807  -0.35424092  0.17712046 -0.         -0.17712046\n",
            "  0.17712046 -0.         -0.35424092  0.35424092  0.         -0.\n",
            " -0.35424092 -0.2656807  -0.         -0.          0.44280115 -0.2656807\n",
            " -0.35424092 -0.6199216  -0.2656807  -0.35424092  0.         -0.17712046\n",
            " -0.         -0.17712046 -0.         -0.          0.70848185 -0.\n",
            "  0.         -0.         -0.         -0.35424092  0.          0.17712046\n",
            "  0.         -0.35424092 -0.         -0.35424092 -0.         -0.17712046\n",
            "  0.44280115  0.         -0.44280115  0.          0.2656807   0.2656807\n",
            " -0.2656807  -0.2656807  -0.44280115 -0.5313614  -0.          0.\n",
            "  0.35424092  0.35424092 -0.17712046  0.5313614  -0.17712046  0.2656807\n",
            " -0.         -0.          0.2656807  -0.          0.          0.17712046\n",
            "  0.08856023  0.2656807   0.35424092 -0.          0.         -0.2656807\n",
            " -0.35424092  0.          0.          0.         -0.         -0.2656807\n",
            " -0.2656807   0.44280115 -0.44280115 -0.         -0.5313614   0.\n",
            "  0.2656807   0.         -0.2656807   0.35424092 -0.         -0.2656807\n",
            " -0.          0.35424092  0.2656807  -0.2656807   0.35424092  0.35424092\n",
            " -0.5313614   0.17712046  0.17712046  0.          0.         -0.\n",
            "  0.35424092 -0.         -0.44280115 -0.          0.          0.\n",
            " -0.17712046  0.17712046 -0.         -0.35424092  0.2656807   0.\n",
            "  0.2656807  -0.         -0.          0.          0.          0.17712046\n",
            "  0.2656807   0.          0.2656807   0.         -0.         -0.\n",
            " -0.          0.44280115 -0.          0.35424092  0.          0.44280115\n",
            "  0.35424092 -0.         -0.         -0.          0.         -0.2656807\n",
            "  0.          0.          0.2656807   0.         -0.2656807  -0.\n",
            "  0.         -0.35424092 -0.2656807  -0.44280115 -0.          0.\n",
            " -0.08856023 -0.35424092 -0.          0.2656807  -0.35424092  0.44280115\n",
            " -0.         -0.44280115  0.2656807  -0.35424092 -0.          0.\n",
            "  0.44280115 -0.         -0.35424092 -0.35424092 -0.17712046  0.2656807\n",
            "  0.          0.35424092 -0.35424092 -0.2656807  -0.35424092 -0.\n",
            " -0.2656807  -0.17712046 -0.44280115 -0.2656807   0.35424092  0.44280115\n",
            "  0.17712046  0.          0.17712046  0.35424092  0.35424092  0.35424092\n",
            " -0.35424092  0.         -0.          0.2656807   0.         -0.\n",
            " -0.         -0.17712046 -0.2656807  -0.44280115  0.         -0.\n",
            " -0.         -0.17712046 -0.44280115  0.        ], shape=(1024,), dtype=float32)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LPqZ_X5pZc-5"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}